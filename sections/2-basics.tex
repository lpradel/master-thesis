\section{Grundlegende Definitionen und Algorithmen}
\label{section:basics}

In diesem Kapitel definieren wir die für unsere Zwecke relevanten Begriffe im Kontext der Clusteranalyse und führen die
wichtigen grundlegenden Algorithmen ein, deren Ideen für uns im Folgenden noch von Bedeutung sein werden. Wir gehen dabei
nach Themengebieten geordnet vor: In Abschnitt~\ref{subsection:basics:clustering} skizzieren wir kurz das
Themengebiet der Clusteranalyse, definieren die üblichen Zielfunktionen und stellen zwei bedeutende Algorithmen vor.
Abschnitt~\ref{subsection:basics:graphs} führt kurz in die Graphentheorie sowie die Clusteranalyse von Graphen ein. In diesem
Abschnitt werden wir zudem verschiedene Optimierungskritierien für die
Clusteranalyse von Graphen herausstellen. Schließlich fassen wir in Abschnitt~\ref{subsection:basics:kernel-spectral} die
wichtigsten Methoden und Algorithmen aus dem Bereich der spektralen Clusteranalyse zusammen und stellen zudem die wesentlichen
Konzepte von Kernel-Methoden vor.

\subsection{Clustering und \texorpdfstring{$k$}{k}-means}
\label{subsection:basics:clustering}

Clusteranalyse oder "`Clustering"' beschäftigt sich mit der Einteilung von Objekten in Gruppen ("`Cluster"'), sodass
sich die Objekte innerhalb eines Clusters gemäß eines bestimmten Optimierungskriteriums ähnlich sind und von Objekten eines
anderen Clusters unterscheiden. Es existieren zahlreiche grundsätzlich verschiedene Ansätze, um Clusteringprobleme zu lösen.
Wir beschränken uns in dieser Arbeit auf \emph{partitionierende} Clusteringprobleme und -verfahren. Bei diesen
soll eine Menge von Objekten, welche der erste Teil der Eingabe ist, gemäß einer Cluster-Zielfunktion möglichst
optimal in genau $k$ Cluster unterteilt werden. Dabei ist $k$ der ganzzahlige zweite Teil der Eingabe, das heißt, die
Anzahl an Clustern muss bei diesen Verfahren vorab festgelegt werden. Die initial gewählten $k$ Zentren werden dann iterativ
verschoben, bis ein gewisses Abbruchkriterium erfüllt ist.

Für die Zielfunktion, welche die Nähe oder Ferne von Punkten zueinander quantifiziert, sind bei Eingabepunkten aus
$\Rd$ Metriken naheliegend. Intuitiv ist dabei die euklidische Distanz, auf der die beiden bekanntesten
Clustering-Problemstellungen basieren. Wir notieren die euklidische Distanz zwischen zwei Punkten $x, y \in \mathbb{R}^d$ mit
$\Euclid{x}{y}$.

\begin{definition}[$k$-median und $k$-means]
\label{def:kmeans-kmedian}
Sei $P \subset \Rd$ und $k \in \mathbb{N}^{+}$. Das $k$-median-Problem besteht darin, eine Menge von $k$ (Cluster-)\emph{Zentren}
$C = \{ c_1, \dots, c_k \}$ mit $c_i \in \Rd$ zu finden, sodass der folgende Term minimal wird:
\[ \sum_{p \in P} \min_{c \in C} \Euclid{p}{c} \]
Das $k$-means-Problem unterscheidet sich nur darin, dass bei diesem die Summe der \emph{quadrierten} euklidischen Distanzen
zum jeweils nächstgelegenen Zentrum minimiert werden soll, das heißt, dass der folgende Term minimiert werden soll:
\[ \sum_{p \in P} \min_{c \in C} \EuclidSquared{p}{c} \]
Beim \emph{gewichteten} $k$-means-Problem werden den Eingabepunkten zusätzlich mit einer Funktion $w : P \rightarrow \mathbb{R}$
Gewichte zugewiesen. Die zu minimierende Zielfunktion lautet dann entsprechend
\[ \sum_{p \in P} \min_{c \in C} w(p) \EuclidSquared{p}{c} \]
\end{definition}
Sowohl das $k$-Median-Problem~\cite{MegiddoS84} als auch das $k$-means-Problem~\cite{AloiseDHP09} sind optimal NP-schwer lösbar.
Typischerweise werden zur Lösung daher approximative oder heuristische Algorithmen eingesetzt. Die bekannteste und bis
heute sehr erfolgreiche Heuristik für das $k$-means-Problem ist der Algorithmus von Lloyd~\cite{Lloyd82}. Wegen seiner
großen Popularität wird der Algorithmus häufig auch als "`$k$-means-Algorithmus"' bezeichnet.
Der Algorithmus wählt initial $k$ zufällige Punkte aus der Eingabemenge als initiale Clusterzentren. Darüber hinaus existieren
zahlreiche weitere Methoden zur initialen Wahl der Zentren, siehe dazu beispielsweise~\cite{CelebiKV13}.
Anschließend wird jedem Punkt das am nächsten gelegene Zentrum zugewiesen. Dadurch entstehen die initialen Cluster
mit ihren jeweiligen Zentren. Im zweiten Schritt wird das neue Zentrum eines jeden Clusters als der geometrische Zentroid
des Clusters gewählt.
Die Wahl des geometrischen Zentroiden als neues Zentrum ist für das 1-means-Problem optimal~\cite{OstrovskyRSS12}.
Die Zuweisung von Punkten zum nächstgelegenen Clusterzentrum und die Berechnung der neuen Zentroiden werden
solange alterniert, bis die Lösung konvergiert, also wenn sich die Zuordnungen der Punkte nicht mehr ändern. In der Praxis
wird gelegentlich auch nach einer festen Anzahl von Iterationen terminiert.

\begin{algorithm}[H]
\label{algo:lloyd}
\caption{Algorithmus von Lloyd}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$}
	\Output{$k$-means-Clustering von $P$}
	\BlankLine
	
	% Algo
	Wähle zufällig $k$ Zentren $c_1^{(0)}, \dots, c_k^{(0)}$ aus $P$\;
	$S_i^{(0)} \leftarrow \{ p \in P : \EuclidSquared{p}{c_i^{(0)}} \leq \EuclidSquared{p}{c_{i'}^{(0)}} \, \forall \, i' \in \{ 1, \dots, k \} \}$\;
	\Repeat{$S_i^{(t)} = S_i^{(t-1)}$}{
		$c_i^{(t)} \leftarrow \frac{1}{|S_i^{(t-1)}|} \sum_{p_j \in S_i^{(t-1)}} p_j $\;
		$S_i^{(t)} \leftarrow \{ p \in P : \EuclidSquared{p}{c_i^{(t)}} \leq \EuclidSquared{p}{c_{i'}^{(t)}} \, \forall \, i' \in \{ 1, \dots, k \} \}$\;
	}
\end{algorithm}

Die asymptotische Laufzeit des Algorithmus beträgt $\BigO{nkdi}$, wobei $i$ die Anzahl an durchgeführten Iterationen ist.
Wenn der Algorithmus konvergiert und nicht durch eine feste Anzahl von Iterationen terminiert wird, wurde ein lokales
Optimum gefunden, welches jedoch im Allgemeinen kein globales Optimum oder eine Approximation eines globalen Optimums ist.
\absatz
Der Algorithmus \kmpp~\cite{ArthurV07} sieht eine alternative Wahl der initialen Clusterzentren in Lloyds Algorithmus vor: er
wählt diese auf einfache, aber dennoch geschickte Art und Weise und führt anschließend die übrigen Schritte von Lloyds
Algorithmus aus.
Dazu wird zunächst ein einzelnes Clusterzentrum $c_1$ zufällig gleichverteilt aus der Eingabe-Punktmenge $P$ gewählt.
Alle weiteren Clusterzentren werden sukzessive nach der folgenden Vorschrift gewählt, bis insgesamt $k$ Zentren gewählt wurden.
Im Weiteren bezeichnen wir mit $D(x)$ für einen Punkt $x$ aus der Eingabe-Punktmenge $P$ die geringste Distanz von $x$ zum
nächstgelegenen bereits gewählten Zentrum. In jeder Iteration wird als nächstes Zentrum $c_i$ der Punkt
$x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$ mit Wahrscheinlichkeit $\frac{D(x')^2}{\sum_{x \in P} D(x)^2}$ gewählt.

\begin{algorithm}[H]
\label{algo:kmeanspp}
\caption{\kmpp}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$}
	\Output{$k$ initiale Clusterzentren für $P$}
	\BlankLine
	
	% Algo
	Wähle $c_1$ zufällig gleichverteilt aus $P$\;
	\For{$i \leftarrow 2$ \KwTo $k$}{
		Wähle den Punkt $x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$ als Zentrum $c_i$ mit Wahrscheinlichkeit $\frac{D(x')^2}{\sum_{x \in P} D(x)^2}$\;
	}
	Führe Lloyds Algorithmus mit den initialen Clusterzentren $c_1, \dots, c_k$ aus.
\end{algorithm}
Die $k$ Zentren, die von \kmpp{} ausgewählt werden, sind eine $\BigO{\log k}$-Approximation für das $k$-means-Problem, die durch
die anschließende Ausführung von Lloyds Algorithmus noch zu einem lokalen Optimum verbessert werden.
\absatz
In der Einleitung haben wir bereits das Konzept der (starken) Kernmenge erwähnt. Diese ist formal folgendermaßen definiert.
\begin{definition}[Starke Kernmenge~\citep{HarPeledM04,Schmidt14}]
\label{def:strong-coreset}
	Sei $P \subset \Rd$ eine endliche Punktmenge und sei $w_1 : P \rightarrow \mathbb{R}$ eine Gewichtsfunktion, die jedem
	Punkt in $P$ ein Gewicht zuordnet. Sei weiterhin $\epsilon \in (0,1)$. Eine endliche Menge $P \subset \Rd$ und eine
	Gewichtsfunktion $w_2 : S \rightarrow \mathbb{R}$ bilden eine \emph{starke} $(1-\epsilon)$-Kernmenge für $P$, wenn
	für alle Mengen von $k$ Zentren $C \subset P$ gilt:
	\[ (1-\epsilon) \sum_{x \in P} w_1(x) \min_{c \in C} \EuclidSquared{x}{c} 
		\leq \sum_{y \in S} w_2(y) \min_{c \in C} \EuclidSquared{y}{c}
		\leq  (1+\epsilon) \sum_{x \in P} w_1(x) \min_{c \in C} \EuclidSquared{x}{c} \]
\end{definition}

Im nächsten Abschnitt betrachten wir das Clusteringproblem auf Graphen, das wir später noch genauer untersuchen wollen.

\subsection{Graphen und Clusteranalyse von Graphen}
\label{subsection:basics:graphs}

Der \emph{Graph} ist in der Informatik eine vielseitig einsetzbare und gut erforschte Datenstruktur. Wir können diverse
Clusteringprobleme nicht nur für eine Eingabepunktmenge formulieren, sondern auch für eine Eingabe, die aus einem Graphen
besteht. Wir beginnen mit der folgenden Definition des Graphen.

\begin{definition}[Graph]
\label{def:graph}
	Sei $V$ eine endliche Menge und $E \subseteq \{ \{ u,v \} \mid u, v \in V, u \neq v \}$.
	Dann heißt das Tupel $G = (V, E)$ ein (endlicher) \emph{Graph} mit \emph{Knotenmenge} $V$ und
	\emph{Kantenmenge} $E$. Ist $e = \{ u,v \} \in E$, dann sagen wir, dass die Kante $e$ des Graphen $G$
	die Knoten $u$ und $v$ \emph{verbindet}. In diesem Fall sind $u$ und $v$ die \emph{Endknoten}
	von $e$.
	Ein Knoten $u \in V$ und eine Kante $e \in V$ heißen \emph{inzident}
	genau dann, wenn $u \in e$. Wir sagen, dass $u$ und ein weiterer Knoten $v \in V$ \emph{adjazent}
	sind genau dann, wenn es eine Kante $e' = \{ u,v \}$ in $E$ gibt.
	Typischerweise bezeichnet $n = \left|V\right|$ die \emph{Knotenzahl} von $G$ und
	$m = \left|E\right|$ die \emph{Kantenzahl} von $G$.
	
	Bei einem 3-Tupel $G' = (V', E', w')$ mit $w' : E \rightarrow \mathbb{N}$ spricht man von einem \emph{gewichteten} Graphen,
	dessen Kanten über ein Gewicht verfügen, das von der Gewichtsfunktion $w'$ abgebildet wird.
\end{definition}
Es gibt zwei grundlegende Datenstrukturen für Graphen: Bei den sogenannten
\emph{Adjazenzlisten} wird für jeden Knoten $v$ im Graphen eine Liste $Adj_v$ gehalten, in der für jeden zu $v$ inzidenten
Knoten ein Eintrag in $Adj_v$ enthalten ist, der den entsprechenden adjazenten Knoten referenziert, sowie gegebenenfalls das
Gewicht der Kante zwischen den beiden Knoten.
Alternativ wird in einer
\emph{Adjazenzmatrix} $Adj^G$ der Größe $\left|V\right| \times \left|V\right|$ an der Stelle $Adj^G_{u,v}$ das Gewicht der Kante
zwischen den Knoten $u$ und $v$ eingetragen, sofern die beiden Knoten durch eine Kante miteinander verbunden sind.
Anderenfalls wird zumeist $-1$ oder $0$ eingetragen. Bei ungewichteten Graphen wird dementsprechend lediglich $0$ oder $1$
eingetragen.
\absatz
Wenn die Eingabe keine Punktmenge, sondern ein (gewichteter) Graph ist, können wir die Ideen der bereits vorgestellten Verfahren
weiterhin anwenden. Wir interessieren uns in diesem Fall für die "`Ähnlichkeit"' von Knotenmengen im Graphen.

\begin{definition}[Graph-Schnitt]
	Sei $G=(V,E,w)$ ein gewichteter Graph. Ein \emph{Schnitt} $C=(S,T)$ von $G$ ist eine Partitionierung von $V$ in die beiden
	Mengen $S$ und $T$, das heißt, dass $V = S \cup T$ und gleichzeitig $S \cap T = \emptyset$.
	Die \emph{Schnittmenge} von $C$ sind die Kanten in $E$, die einen
	Endpunkt in $S$ und den anderen Endpunkt in $T$ haben, das heißt, formal ist die Schnittmenge definiert als
	\[ \{ (u,v) \in E \mid u \in S, v \in T \}. \]
	Das Gewicht oder der Wert eines Schnittes ist die Summe der Kantengewichte der Schnittmenge. Wir verwenden die folgende
	Notation:
	\[ w_{cut}(S,T) = \sum_{u \in S, v \in T} w((u,v)) \]
	Falls der Graph in Form einer Adjazenzmatrix $Adj^G$ vorliegt, lautet die Berechnungsvorschrift entsprechend:
	\[ w_{cut}(S,T) = \sum_{u \in S, v \in T} Adj^G_{u,v} \]
\end{definition}
Für eine direkte Analogie zum $k$-means-Problem wäre ein Schnitt- beziehungsweise Partitionierungs-Begriff wünschenswert,
der eine $k$-fache Partitionierung der Knotenmenge erlaubt. Diese lautet wie folgt.

\begin{definition}[$k$-Graphpartitionierung]
\label{def:graphpartitioning}
	Sei $G=(V,E,w)$ ein gewichteter Graph. Für zwei Mengen $A, B \subseteq V$ definieren wir:
	\[ w(A,B) = \sum_{u \in A, v \in B} w((u,v)) \]
	Das $k$-Graphpartitionierungsproblem besteht darin, eine Partitionierung der Knotenmenge $V$ in $k$ disjunkte Teilmengen
	$V_1, \dots, V_k$ mit $\bigcup_{i \in \{1, \dots, k \}} V_i = V$ zu ermitteln, sodass sich die Knoten innerhalb einer
	Partition bezüglich einer Ähnlichkeitsrelation möglichst ähnlich sind und sich die Knoten unterschiedlicher Partitionen
	bezüglich der Ähnlichkeitsrelation möglichst stark voneinander unterscheiden~\cite{KernighanL70}.
	
	Beim Clustering von Punktmengen lagen für die Unähnlichkeitsrelation Metriken nahe, im Falle der Graphpartitionierung
	existiert eine Reihe von Optimierungskriterien, von denen wir im Folgenden die verbreitetsten einführen.
	
	\begin{enumerate}
		\item 	\textbf{Ratio Association}. Bei der Ratio Association sollen die Intra-Clusterabstände relativ zur jeweiligen
				Clustergröße maximiert werden:
				\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|} \]
		\item 	\textbf{Ratio Cut}. Beim Ratio Cut wird das Gewicht des Schnitts zwischen jeweils einem Cluster und allen
				anderen Punkten im Graphen minimiert:
				\[ \min \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{\left|V_c\right|} \]
		\item 	\textbf{Kernighan-Lin}. Bei dem in~\cite{KernighanL70} vorgestellten Optimierungskriterium werden die
				Intra-Clusterabstände ähnlich der Ratio Association minimiert, allerdings müssen alle Partitionen hier zusätzlich
				dieselbe Größe haben. Die hier vorgestellte Zielfunktion ist von $2$ auf $k$ Partitionen verallgemeinert:
				\[ \min \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{\left|V_c\right|} \textrm{ s.t. } 
				   \left|V_c\right| = \frac{\left|V\right|}{k} \, \forall \, c \in \{ 1, \dots, k \} \]
		\item 	\textbf{Normalized Cut}. Ziel des Normalized Cut ist wie beim Ratio Cut die Minimierung des Schnitts von einem
				Cluster mit den übrigen Punkten im Graphen, jedoch relativ zum Schnitt des Clusters mit \emph{allen}
				Knoten im Graphen. Der Normalized Cut oder kurz NCut ist in der Bild-Segmentierung recht verbreitet~\cite{ShiM00}.
				Wir betrachten hier ebenfalls die auf $k$ Partitionen verallgemeinerte Zielfunktion.
				\[ \min \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{w(V_c, V)} \]
	\end{enumerate}
\end{definition}
Im nächsten Abschnitt betrachten wir zwei Techniken, mit denen Limitierungen hinsichtlich der Qualität der erzielten Clusterings
sowie der Komplexität der Berechnungen verbessert werden können.

\subsection{Kernel-Methoden und spektrales Clustering}
\label{subsection:basics:kernel-spectral}

Legt man die $k$-means Zielfunktion zu Grunde, können im Ursprungsraum $\Rd$ nur Cluster berechnet werden, die \emph{linear
separierbar} sind. Ein illustratives Beispiel im $\mathbb{R}^2$ sind zwei konzentrisch angeordnete ringförmige Punktmengen.
Die intuitiv optimalen Cluster sind die beiden jeweiligen Ringe, es ist jedoch im $\mathbb{R}^2$ nicht
möglich, Clusterzentren anzugeben, die den zweidimensionalen Raum in Ringe partitionieren. Die Beschränkung liegt darin, dass
wir die Partitionierung oder Separierung durch Hyperebenen vornehmen. Im Ursprungsraum können wir daher nur linear separierbare
Cluster bestimmen. Wir betrachten nun ein Verfahren, mit dem sich diese Beschränkung umgehen lässt.

\subsubsection{Kernel-\texorpdfstring{$k$}{k}-means}

Unsere fundamentale Motivation für diesen Abschnitt ist Covers Theorem~\cite{Cover65}, welches informell zusammengefasst besagt,
dass eine zufällige Eingabepunktmenge im $d$-dimensionalen Raum mit hoher Wahrscheinlichkeit in einen höherdimensionalen
Raum (zum Beispiel der Dimension $D \gg d$) abgebildet linear separierbar ist.

Für Klassifizierungs- und insbesondere Clusteringprobleme benötigen wir sowohl im Ursprungsraum als auch im abgebildeten Raum
ein algebraisches Maß für Unähnlichkeit. Üblicherweise wird dabei eine auf \emph{Skalarprodukten} basierende Metrik eingesetzt.
Wir beziehen uns im Weiteren auf die folgende Definition von \emph{euklidischen Vektorräumen}, also Vektorräumen mit einem
reellen Skalarprodukt.

\begin{definition}[Euklidischer Vektorraum]
\label{def:innerproductspace}
	Sei $V$ ein Vektorraum über $\mathbb{R}$. Ein reelles Skalarprodukt auf $V$ ist eine Abbildung
	$\innerproduct{\cdot}{\cdot} : V \times V \rightarrow \mathbb{R}$, die den folgenden Eigenschaften genügt:
	\begin{enumerate}
		\item 	$\innerproduct{a}{b} = \innerproduct{b}{a} \, \forall \, a, b \in V$
		\item 	$\innerproduct{\lambda a + \mu b}{c} = \lambda \innerproduct{a}{c} + \mu \innerproduct{b}{c} \,
				\forall \, a,b,c \in V \textrm{ und } \lambda, \mu \in \mathbb{R}$
		\item 	$\innerproduct{a}{a} \geq 0 \textrm{ und } \innerproduct{a}{a} = 0 \Leftrightarrow a = 0 \, \forall \, a \in V$
	\end{enumerate}
	Ein $\mathbb{R}$-Vektorraum mit einem reellen Skalarprodukt heißt \emph{euklidischer Vektorraum}.
\end{definition}
Die $k$-means-Zielfunktion kann auf euklidische Vektorräume erweitert werden, indem wir für einen euklidischen Vektorraum
$\mathcal{V}$ eine Norm $\lVert \cdot \rVert : \mathcal{V} \rightarrow \mathbb{R}$ über das Skalarprodukt gemäß der folgenden
Abbildungsvorschrift für jedes $\mathpzc{x} \in \mathcal{V}$ definieren:
$\lVert \mathpzc{x} \rVert = \sqrt{\innerproduct{\mathpzc{x}}{\mathpzc{x}}}$.
Das auf Skalarprodukten basierte Analog zu Definition~\ref{def:kmeans-kmedian} des $k$-means-Problems lautet dann wie folgt:

\begin{definition}[$k$-means in euklidischen Vektorräumen~\cite{Schmidt14}]
\label{def:kmeans-kmedian-innerproduct}
Sei $\mathcal{V}$ ein euklidischer Vektorraum mit der Norm $\lVert \cdot \rVert : \mathcal{V} \rightarrow \mathbb{R}$
und sei $\mathcal{P} \subset \mathcal{V}$ sowie $k \in \mathbb{N}^{+}$.
Das $k$-means-Problem in $\mathcal{V}$ besteht darin, eine Menge von $k$ Clusterzentren
$\mathcal{C} = \{ \mathpzc{c}_1, \dots, \mathpzc{c}_k \}$ mit $\mathpzc{c}_i \in V$ zu finden, sodass der folgende Term
minimal wird:
\[ \sum_{\mathpzc{p} \in \mathcal{P}} \min_{\mathpzc{c} \in \mathcal{C}} \EuclidSquared{\mathpzc{p}}{\mathpzc{c}} \]
Die gewichtete Variante des $k$-means-Problems in euklidischen Vektorräumen lautet entsprechend wie folgt:
\[ \sum_{\mathpzc{p} \in \mathcal{P}} \min_{\mathpzc{c} \in \mathcal{C}} w(\mathpzc{p})
	\EuclidSquared{\mathpzc{p}}{\mathpzc{c}} \]
\end{definition}
Damit ist es nun möglich, ein intuitives Verfahren zum Bestimmen von Clusterings mit nicht-linearen Separatoren anzugeben.
Wir bilden zunächst mit einer Abbildung $\phi : \mathcal{X} \rightarrow \mathcal{V}$ unsere Eingabepunkte aus ihrem Ursprungsraum
$\mathcal{X}$ (also beispielsweise $\Rd$) auf einen $D$-dimensionalen (üblicherweise $D \gg d$) euklidischen
Vektorraum $\mathcal{V}$ ab, und berechnen in diesem mit Lloyds Algorithmus oder einem anderen Clusteringverfahren ein
Clustering, welches im Ursprungsraum $\mathcal{X}$ im Allgemeinen nicht-linear separierten Clustern entspricht.
Außerdem ist es mit einem "`Trick"', den wir im Folgenden erläutern, möglich, implizit auf einen unendlich dimensionalen
Vektorraum (einen Funktionenraum), abzubilden.
Bei diesem Verfahren wird entsprechend der folgende Term minimiert:
\[ \sum_{p \in P} \min_{c \in C} \EuclidSquared{\phi(p)}{c} \]
Die Zentroid-Berechnung zur Bestimmung eines Clusterzentrums $c_i$ im Sinne von Lloyds Algorithmus lautet dann
\[ c_i = \frac{\sum_{p \in S_i} \phi(p)}{\left|S_i\right|} \]
wobei $S_i$ die Menge aller dem Clusterzentrum $c_i$ zugeordneten Punkte ist. Zunächst haben wir uns damit nicht-linear separierte
Cluster "`erkauft"', der Preis, den wir dafür zahlen, besteht jedoch in höherem Rechenaufwand, da wir einerseits die Punkte
in den jeweils höherdimensionalen Raum abbilden müssen und andererseits die Berechnung der (euklidischen) Distanzen aufwändiger
wird; diese benötigt im Ursprungsraum Zeit $\mathcal{O}(d)$ und im höherdimensionalen Raum $\mathcal{O}(D)$, sofern
dieser endlich dimensional ist. Der Mehraufwand
für die Distanzberechnung fällt dabei stärker ins Gewicht, da wir die Distanzberechnung beispielsweise bei Lloyds Algorithmus
in jeder Schleifeniteration durchführen.
\absatz
Dieses Problem wurde erstmals im Zusammenhang mit \emph{Support Vector Machines} (kurz SVMs) gelöst~\cite{BoserGV92}.
Die Lösung mittels des sogenannten \emph{Kernel-Tricks} kann in den Clustering-Kontext übertragen werden~\cite{Schmidt14}.
Wir skizzieren dies kurz.
Zunächst halten wir eine wichtige Beobachtung hinsichtlich der (quadrierten) euklidischen Distanz $\EuclidSquared{\phi(p)}{c}$
zwischen dem abgebildeten Punkt und einem Clusterzentrum fest. Diese lässt sich wie folgt ausschließlich über Skalarprodukte
berechnen (wobei auch hier wieder $S_c$ die dem Clusterzentrum $c$ zugeordnete Punktmenge ist):
\begin{align}
\label{formula:kmeans-innerproduct}
	\EuclidSquared{\phi(p)}{c} &= \bigg\lVert \phi(p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \phi(x) \bigg\rVert^2 \notag \\
	&= \bigg\langle \phi(p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \phi(x),
		\phi(p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \phi(x) \bigg\rangle \notag \\
	&= \innerproduct{\phi(p)}{\phi(p)} - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \innerproduct{\phi(p)}{\phi(x)}
		- \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \innerproduct{\phi(x)}{\phi(p)} \notag \\
		& \hspace{0.4cm} + \frac{1}{\left|S_c\right|^2} \sum_{x,y \in S_c} \innerproduct{\phi(x)}{\phi(y)}
\end{align}
Diese Beobachtung allein hilft uns noch nicht weiter, da auch die Berechnung des Skalarproduktes im höherdimensionalen
Raum einen asymptotischen Aufwand von $\mathcal{O}(D)$ hätte. An dieser Stelle kommt nun der in~\cite{BoserGV92} vorgestellte
\emph{Kernel-Trick} zum Tragen. Durch den Einsatz von sogenannten \emph{Kernelfunktionen} lässt sich dieser Mehraufwand
umgehen. Dabei handelt es sich um positiv definite Abbildungen $\kappa : X \times X \rightarrow \mathbb{R}$, die bei Eingabe von
zwei Punkten aus dem Ursprungsraum $X$ das Skalarprodukt der in den höherdimensionalen Raum abgebildeten Punkte berechnen:
\[ \kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)} \]
Der Kernel-Trick besteht also darin, dass sich die Distanzberechnung auf Skalarprodukte von Eingabepunkten reduzieren lässt, für
die keine explizite Kenntnis der Abbildung $\phi$ oder der abgebildeten Punkte $\phi(p)$ nötig ist. Dies können wir umsetzen,
indem wir insgesamt $\mathcal{O}(n^2)$ Skalarprodukte berechnen oder direkt eine Kernelfunktion $\kappa$ verwenden.
Mit der Kernelfunktion $\kappa$ können wir die Distanzberechnung aus ~\ref{formula:kmeans-innerproduct} weiter vereinfachen:
\begin{align}
\label{formula:kernel-kmeans}
	\EuclidSquared{\phi(p)}{c} = \kappa(p,p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \kappa(p, x)
		- \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \kappa(x, p)
		+ \frac{1}{\left|S_c\right|^2} \sum_{x,y \in S_c} \kappa(x, y)
\end{align}

\begin{table}[t]
\centering
\begin{tabular}{@{}lr@{}} \toprule
	\textbf{Linear-Kernel} & $\kappa(x_i, x_j) = x_i \cdot x_j + \gamma $ \\
	\textbf{Polynom-Kernel} & $\kappa(x_i, x_j) = \left( x_i \cdot x_j + \gamma \right)^\delta$ \\
	\textbf{Gauss-/RBF-Kernel} & $\kappa(x_i, x_j) = \exp \left( - \frac{\EuclidSquared{x_i}{x_j}}{2 \sigma^2} \right)$ \\
	\textbf{Sigmoid-Kernel} & $\kappa(x_i, x_j) = \tanh(\alpha(x_i \cdot x_j) + \theta) $ \\ \bottomrule
\end{tabular}
\caption{Beispiele für häufig eingesetzte Kernelfunktionen im Maschinellen Lernen~\cite{HofmannSS08}.}
\label{tbl:kernel-functions}
\end{table}
Tabelle~\ref{tbl:kernel-functions} bietet eine Übersicht über häufig eingesetzte Kernelfunktionen im Maschinellen
Lernen~\cite{HofmannSS08}. Insbesondere
die Gauss-Kernelfunktion und die Polynom-Kernelfunktion werden auch bei Support-Vektor-Maschinen gerne verwendet.
\absatz
Eine auf Kernelfunktionen basierende Variante von (Lloyds) Algorithmus~\ref{algo:lloyd} folgt mit \ref{formula:kernel-kmeans}
unmittelbar. Der Algorithmus wird üblicherweise kurz "`Kernel-$k$-means-Algorithmus"' genannt.
Der Vorfaktor für gewichtetes $k$-means hat auf die Berechnungsvorschriften keine weiteren Auswirkungen,
Algorithmus~\ref{algo:kernel-lloyd} kann dementsprechend für gewichtetes $k$-means erweitert werden. Der Algorithmus
lässt sich mit einer asymptotischen Laufzeit von $\mathcal{O}(n^2 (\tau + d))$ implementieren~\cite{DhillonGK04,DhillonGK07},
wobei $\tau$ die Anzahl an Iterationen der äußeren Schleife ist.
%\absatz
\begin{algorithm}[t]
\caption{Kernel-$k$-means}
\label{algo:kernel-lloyd}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}, \kappa : X \times X \rightarrow \mathbb{R}$, initiales Clustering $C_1, \dots, C_k$ }
	\Output{$k$-means-Clustering von $P$}
	\BlankLine
	
	% Algo
	%Wähle zufällig $k$ Zentren $c_1^{(0)}, \dots, c_k^{(0)}$ aus $P$ oder $\Rd$\;
	$t \leftarrow 0$\;
	\Repeat{\textnormal{Konvergenz oder} $t > t_{max}$}{
		Berechne $\EuclidSquared{\phi(p)}{c_i^{(t)}}$ für alle $p \in P$ und $i \in \{1, \dots, k\}$ mit~\ref{formula:kernel-kmeans}\;
		\ForEach{$p \in P$}{
			$c^*(p) \leftarrow \argmin_i\left( \bigg\lVert \phi(p) - c_i^{(t)} \bigg\rVert \right)$\;
		}
		\For{$i \leftarrow 1$ \KwTo $k$}{
			%$S_i^{(t)} \leftarrow \{ p \mid c^*(p) = i \}$\;
			%$c_i^{(t)} \leftarrow \frac{1}{|S_i^{(t)}|} \sum_{p_j \in S_i^{(t)}} p_j $\;
			$C_{i}^{t} \leftarrow \{ p \mid c^*(p) = i \}$\;
		}
		$t \leftarrow t + 1$\;
	}
\end{algorithm}

Im nächsten Unterabschnitt führen wir Verfahren ein, die mit Techniken aus der linearen Algebra das Graphclusteringproblem
lösen und in Kombination mit dem hier vorgestellten Algorithmus eine robuste Grundlage für unsere Zwecke bilden.

\subsubsection{Spektrale Clustering Methoden}

Beim spektralen Clustering besteht die Eingabe aus einem Graphen, dessen Knoten geclustert werden sollen. Informationen
über die "`Ähnlichkeit"' von Objekten werden in Form eines Kantengewichts der Kante zwischen den jeweiligen Objekten
realisiert. Wir werden im Weiteren insbesondere Eigenvektoren und zugehörige Eigenwerte von Matrizen betrachten und rufen uns
daher kurz die Definition dieser in Erinnerung.

\begin{definition}[Eigenvektor und Eigenwert]
\label{def:eigenvector-eigenvalue}
	Sei $A$ eine quadratische Matrix. Das \emph{Eigenwertproblem} besteht darin, eine Zahl $\lambda$ und einen
	dazugehörigen Vektor $\vec{x} \neq \vec{0}$ zu finden, für die gilt:
	\[ A \vec{x} = \lambda \vec{x} \]
	Die reelle oder komplexe Zahl $\lambda$ heißt \emph{Eigenwert} von $A$, der Vektor $\vec{x}$ heißt Eigenvektor von $A$.
\end{definition}

Wenn $G$ der Graph ist, durch den die Eingabemenge repräsentiert wird, dann ist die Adjazenzmatrix $Adj^G$ des
Graphen der Ausgangspunkt der spektralen Clusteranalyse. Bevor wir mit konkreten Verfahren der spektralen Clusteranalyse
beginnen können, benötigen wir einige grundlegende Definitionen der (spektralen) Graphentheorie.

\begin{definition}[Grad-Matrix]
\label{def:degree-matrix}
	Sei $G = (V,E)$ ein Graph mit $\left| V \right| = n$ Knoten. Die \emph{Grad-Matrix} von $G$ ist die
	$n \times n$-Diagonalmatrix $D$, die folgendermaßen definiert ist:
	\[ D_{i,j} = 	\begin{cases}
						\deg(v_i) & \textrm{falls } i = j, \\
						0 & \textrm{sonst}
					\end{cases}
	\]
	Dabei ist $\deg(v_i)$ die Anzahl an Kanten, die zum Knoten $v_i$ inzident sind. Diese Anzahl wird auch
	\emph{Grad} des Knotens $v_i$ genannt.
\end{definition}

Mit Hilfe der Grad-Matrix eines Graphen lässt sich die sogenannte \emph{Laplace-Matrix} des Graphen berechnen, welche eine
Matrix-Repräsentation von Graphen ist, die aufschlussreiche Informationen über die Struktur und Beschaffenheit des Graphen
erlaubt. Sie ist wie folgt definiert:

\begin{definition}[Laplace-Matrix~\cite{Luxburg07}]
\label{def:laplace-matrix}
	Sei $G=(V,E)$ ein Graph ohne Schleifen mit Grad-Matrix $D$ und (gewichteter) Adjazenzmatrix $Adj^G$.
	Die (nicht-normalisierte) \emph{Laplace-Matrix} $L$ von $G$ ist definiert als:
	\[ L = D - Adj^G \]
	Die Einträge der Matrix sind dementsprechend folgendermaßen definiert:
	\[ L_{i,j} = 	\begin{cases}
						\deg(v_i) & \textrm{falls } i = j, \\
						-1 & \textrm{falls } i \neq j \textrm{ und $v_i$ inzident zu $v_j$ ist}, \\
						0 & \textrm{sonst}
					\end{cases}
	\]
	Es existieren zwei normalisierte Formen der Laplace-Matrix, die \emph{normalisierte} Laplace-Matrizen genannt werden.
	Sie sind wie folgt definiert:
	\begin{align*}
		\mathcal{L}_{sym} &= D^{-\frac{1}{2}} L D^{-\frac{1}{2}} \\
		\mathcal{L}_{rw} &= D^{-1} L
	\end{align*}
\end{definition}
Die Eigenwerte der Laplace-Matrizen eines Graphen geben beispielsweise Aufschluss über die \emph{Zusammenhangskomponenten} eines
Graphen. Eine Zusammenhangskomponente definiert ist als Teilgraph, in dem jedes Paar von Knoten über einen Pfad
miteinander verbunden ist. Das folgende Lemma illustriert die Zusammenhangsinformationen, die sich aus der Laplace-Matrix
ablesen lassen:

\begin{lemma}[Zusammenhang und Laplace-Matrix~\cite{Luxburg07}]
\label{lemma:connectivity-laplace-spectrum}
	Gegeben sei $G=(V,E)$ ein schleifenloser Graph mit nicht-negativen Gewichten und Laplace-Matrix $L$. Die Anzahl an
	Zusammenhangskomponenten von $G$ ist gleich der Anzahl an Eigenwerten von $L$, die gleich Null sind.
\end{lemma}
Da die Eigenwerte einer Matrix auch "`Spektrum"' genannt werden und man bei den hier vorgestellten Methoden die Eigenwerte
der Laplace-Matrix nutzt, wird diese Technik "`spektrale Clusteranalyse"' genannt.
\absatz
Die wichtigsten Algorithmen, die auf dem Spektrum der Laplace-Matrix der Eingabe basieren, wollen wir im Folgenden vorstellen.
Wir beginnen mit den Algorithmen~\ref{algo:non-normalized-spectral-clustering}
und~\ref{algo:normalized-spectral-clustering-shi-malik}, welche die normalisierten Laplace-Matrizen verwenden und
in den jeweils zitierten Papieren vorgestellt wurden.

\begin{algorithm}
\caption{Nicht-normalisiertes spektrales Clustering~\cite{Luxburg07}}
\label{algo:non-normalized-spectral-clustering}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{Gewichtete Adjazenzmatrix $Adj^G \in \mathbb{R}^{n \times n}, k \in \mathbb{N}^{+}$}
	\Output{$k$ Cluster $C_1, \dots, C_k$}
	\BlankLine
	
	% Algo
	Berechne die nicht-normalisierte Laplace-Matrix $L$ des Graphen $G$ mit $Adj^G$\;
	Bestimme Eigenvektoren $u_1, \dots, u_k$ zu den $k$ kleinsten Eigenwerten von $L$\;
	Sei $U \in \mathbb{R}^{n \times k}$ die Matrix mit $u_1, \dots u_k$ als Spaltenvektoren\;
	Für $i \leftarrow \{ 1, \dots, n \}$ sei $y_i \in \mathbb{R}^k$ der Vektor der $i$-ten Reihe von $U$\;
	Clustere die Punkte $(y_i)_{i \in \{1, \dots, n\}}$ mit einem $k$-means-Algorithmus in $C_1, \dots, C_k$
\end{algorithm}

\begin{algorithm}
\caption{Normalisiertes spektrales Clustering~\cite{ShiM00}}
\label{algo:normalized-spectral-clustering-shi-malik}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{Gewichtete Adjazenzmatrix $Adj^G \in \mathbb{R}^{n \times n}, k \in \mathbb{N}^{+}$}
	\Output{$k$ Cluster $C_1, \dots, C_k$}
	\BlankLine
	
	% Algo
	Berechne die normalisierte Laplace-Matrix $\mathcal{L}_{rw}$ des Graphen $G$ mit $Adj^G$\;
	Bestimme Eigenvektoren $u_1, \dots, u_k$ zu den $k$ kleinsten Eigenwerten von $\mathcal{L}_{rw}$\;
	Sei $U \in \mathbb{R}^{n \times k}$ die Matrix mit $u_1, \dots u_k$ als Spaltenvektoren\;
	Für $i \leftarrow \{ 1, \dots, n \}$ sei $y_i \in \mathbb{R}^k$ der Vektor der $i$-ten Reihe von $U$\;
	Clustere die Punkte $(y_i)_{i \in \{1, \dots, n\}}$ mit einem $k$-means-Algorithmus in $C_1, \dots, C_k$
\end{algorithm}
Der in~\cite{ShiM00} vorgestellte Algorithmus sieht in seiner ursprünglichen Form zunächst eine Bipartitionierung anhand des
Eigenvektors mit dem zweitkleinsten Eigenwert vor,
die anschließend gegebenenfalls weiter partitioniert wird. Dieses Vorgehen ist jedoch analog zu dem hier vor uns angegebenen
Algorithmus.

\begin{algorithm}[H]
\label{algo:normalized-spectral-clustering-ng-jordan-weis}
\caption{Normalisiertes spektrales Clustering~\cite{NgJW01}}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{Gewichtete Adjazenzmatrix $Adj^G \in \mathbb{R}^{n \times n}, k \in \mathbb{N}^{+}$}
	\Output{$k$ Cluster $C_1, \dots, C_k$}
	\BlankLine
	
	% Algo
	Berechne die normalisierte Laplace-Matrix $\mathcal{L}_{sym}$ des Graphen $G$ mit $Adj^G$\;
	Bestimme Eigenvektoren $u_1, \dots, u_k$ zu den $k$ kleinsten Eigenwerten von $\mathcal{L}_{sym}$\;
	Sei $U \in \mathbb{R}^{n \times k}$ die Matrix mit $u_1, \dots u_k$ als Spaltenvektoren\;
	Sei $T \in \mathbb{R}^{n \times k}$ die Matrix mit den Einträgen
	$ T_{i,j} = \frac{U_{i,j}}{\left(\sum_k U_{i,k}^2\right)^{\frac{1}{2}}} $ \;
	Für $i \leftarrow \{ 1, \dots, n \}$ sei $y_i \in \mathbb{R}^k$ der Vektor der $i$-ten Reihe von $T$\;
	Clustere die Punkte $(y_i)_{i \in \{1, \dots, n\}}$ mit einem $k$-means-Algorithmus in $C_1, \dots, C_k$
\end{algorithm}
Der Algorithmus aus~\cite{NgJW01} ist dem von~\cite{ShiM00} sehr ähnlich, verwendet jedoch die normalisierte Laplace-Matrix
$\mathcal{L}_{sym}$. Außerdem ist für diesen Algorithmus eine zusätzliche Normalisierung nötig, deren Details in~\cite{Luxburg07}
erläutert werden.
\absatz
Das Berechnen aller Eigenvektoren benötigt asymptotischen Rechenaufwand von $\mathcal{O}(n^3)$, was für viele Anwendungen
deutlich zu viel Rechenzeit ist. Beschränkt man sich auf die Berechnung von wenigen oder sogar nur einem einzelnen Eigenvektor,
lässt sich die Komplexität signifikant verringern, beispielsweise unter Einsatz von Lanczos Algorithmus~\cite{Lanczos50}.
Da für viele spektrale Clusteringalgorithmen die Berechnung der Eigenvektoren jedoch nach wie vor ein großer Flaschenhals ist,
betrachten wir im nächsten Kapitel einen Ansatz, mit dem die Berechnung von Eigenvektoren für Graphclusterings prinzipiell
vermieden werden kann.