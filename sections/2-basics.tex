\section{Grundlegende Definitionen und Algorithmen}
\label{section:basics}

In diesem Kapitel definieren wir die für unsere Zwecke relevanten Begriffe im Kontext der Clusteranalyse und führen die
wichtigen grundlegenden Algorithmen ein, deren Ideen für uns im Folgenden noch von Bedeutung sein werden. Wir gehen dabei
nach Themengebieten geordnet vor: In Abschnitt~\ref{subsection:basics:clustering} skizzieren wir kurz das
Themengebiet der Clusteranalyse, definieren die üblichen Zielfunktionen und stellen zwei bedeutende Algorithmen vor.
Abschnitt~\ref{subsection:basics:graphs} führt kurz in die Graphentheorie sowie die Clusteranalyse von Graphen ein. In diesem
Abschnitt werden wir zudem die von der klassischen Clusteranalyse sehr unterschiedlichen Optimierungskritierien für die
Clusteranalyse von Graphen herausstellen. Schließlich fassen wir in Abschnitt~\ref{subsection:basics:kernel-spectral} die
wichtigsten Methoden und Algorithmen aus dem Bereich der spektralen Clusteranalyse zusammen und stellen zudem die wesentlichen
Konzepte von Kernel-Methoden vor.

\subsection{Clustering und \texorpdfstring{$k$}{k}-means}
\label{subsection:basics:clustering}

Clusteranalyse oder "`Clustering"' beschäftigt sich mit der Einteilung von Objekten in Gruppen ("`Cluster"'), sodass
sich die Objekte innerhalb eines Clusters gemäß eines bestimmten Optimierungskriteriums ähnlich sind und von Objekten eines
anderen Clusters unterscheiden. Es existieren zahlreiche grundsätzlich verschiedene Ansätze, Clusteringprobleme zu lösen.
Wir beschränken uns in dieser Arbeit auf \emph{partitionierende} Clusteringprobleme und -verfahren. Bei diesen
soll eine Menge von $d$-dimensionalen Punkten, welche der erste Teil der Eingabe ist, gemäß einer Cluster-Zielfunktion möglichst
optimal in genau $k$ Cluster unterteilt werden, wobei $k$ der ganzzahlige zweite Teil der Eingabe ist.

Für die Zielfunktion, welche die Nähe oder Ferne von Punkten zueinander quantifiziert, sind bei Eingabepunkten aus
$\Rd$ Metriken naheliegend. Intuitiv ist dabei die euklidische Distanz, welche als Zielfunktion für die beiden bekanntesten
Clustering-Problemstellungen dient.

\begin{definition}[$k$-median und $k$-means]
\label{def:kmeans-kmedian}
Sei $P \subset \Rd$ und $k \in \mathbb{N}^{+}$. Das $k$-median-Problem besteht darin, eine Menge von $k$ (Cluster-)\emph{Zentren}
$C = \{ c_1, \dots, c_k \}$ mit $c_i \in \Rd$ zu finden, sodass der folgende Term minimal wird:
\[ \sum_{p \in P} \min_{c \in C} \Euclid{p}{c} \]
Das $k$-means-Problem unterscheidet sich nur darin, dass bei diesem die Summe der \emph{quadrierten} euklidischen Distanzen
zum jeweils nächstgelegenen Zentrum minimiert werden soll, das heißt, dass der folgende Term minimiert werden soll:
\[ \sum_{p \in P} \min_{c \in C} \EuclidSquared{p}{c} \]
Beim \emph{gewichteten} $k$-means-Problem werden den Eingabepunkten zusätzlich mit einer Funktion $w : P \rightarrow \mathbb{R}$
Gewichte zugewiesen. Die zu minimierende Zielfunktion lautet dann entsprechend
\[ \sum_{p \in P} \min_{c \in C} w(p) \EuclidSquared{p}{c} \]
\end{definition}
Sowohl das $k$-Median-Problem~\cite{MegiddoS84} als auch das $k$-means-Problem~\cite{AloiseDHP09} sind optimal NP-schwer lösbar.
Typischerweise werden zur Lösung daher approximative oder heuristische Algorithmen eingesetzt. Die bekannteste und bis
heute sehr erfolgreiche Heuristik für das $k$-means-Problem ist der Algorithmus von Lloyd~\cite{Lloyd82}. Wegen seiner
großen Popularität wird der Algorithmus häufig auch als "`$k$-means-Algorithmus"' bezeichnet.
Der Algorithmus wählt initial $k$ zufällige Punkte aus der Eingabemenge oder sogar beliebige Punkte aus $\Rd$ als initiale
Clusterzentren. Anschließend wird jedem Punkt das am nächsten gelegene Zentrum zugewiesen. Dadurch entstehen die initialen Cluster
mit ihren jeweiligen Zentren. Im zweiten Schritt wird das neue Zentrum eines jeden Clusters als der geometrische Zentroid
des Clusters gewählt. Die Zuweisung von Punkten zum nächstgelegenen Cluster und die Neuberechnung der neuen Zentren werden
solange alterniert, bis die Lösung konvergiert, also wenn sich die Zuordnungen der Punkte nicht mehr ändern. In der Praxis
wird gelegentlich auch nach einer festen Anzahl von Iterationen terminiert.

\begin{algorithm}[H]
\label{algo:lloyd}
\caption{Algorithmus von Lloyd}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$}
	\Output{$k$-means-Clustering von $P$}
	\BlankLine
	
	% Algo
	Wähle zufällig $k$ Zentren $c_1^{(0)}, \dots, c_k^{(0)}$ aus $P$ oder $\Rd$\;
	$S_i^{(0)} \leftarrow \{ p \in P : \EuclidSquared{p}{c_i^{(0)}} \leq \EuclidSquared{p}{c_{i'}^{(0)}} \, \forall \, i' \in \{ 1, \dots, k \} \}$\;
	\Repeat{$S_i^{(t)} = S_i^{(t-1)}$}{
		$c_i^{(t)} \leftarrow \frac{1}{|S_i^{(t-1)}|} \sum_{p_j \in S_i^{(t-1)}} p_j $\;
		$S_i^{(t)} \leftarrow \{ p \in P : \EuclidSquared{p}{c_i^{(t)}} \leq \EuclidSquared{p}{c_{i'}^{(t)}} \, \forall \, i' \in \{ 1, \dots, k \} \}$\;
	}
\end{algorithm}

Die asymptotische Laufzeit des Algorithmus beträgt $\BigO{nkdi}$, wobei $i$ die Anzahl an durchgeführten Iterationen ist.
Wenn der Algorithmus konvergiert und nicht durch eine feste Anzahl von Iterationen terminiert wird, wurde ein lokales
Optimum gefunden, welches jedoch im Allgemeinen kein globales Optimum oder eine Approximation eines globalen Optimums ist.
Die Güte des berechneten Clusterings hängt maßgeblich von der initialen Wahl der Cluster ab. Der Algorithmus
\kmpp~\cite{ArthurV07} setzt genau an dieser Stelle an: er berechnet auf einfache, aber dennoch geschickte Art und Weise
die initialen Cluster und führt anschließend mit diesen die übrigen Schritte von Lloyds Algorithmus durch.
Der Algorithmus wählt zunächst ein einzelnes Clusterzentrum $c_1$ zufällig gleichverteilt aus der Eingabe-Punktmenge $P$ und
wählt alle weiteren Clusterzentren sukzessive nach der folgenden Vorschrift, bis insgesamt $k$ Zentren gewählt wurden.
Im Weiteren bezeichnen wir mit $D(x)$ für einen Punkt $x$ aus der Eingabe-Punktmenge $P$ die geringste Distanz von $x$ zum
nächstgelegenen bereits gewählten Zentrum. In jeder Iteration wird als nächstes Zentrum $c_i$ der Punkt
$x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$ mit Wahrscheinlichkeit $\frac{D(x')^2}{\sum_{x \in P} D(x)}$ gewählt.

\begin{algorithm}[H]
\label{algo:kmeanspp}
\caption{\kmpp}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$}
	\Output{$k$ initiale Clusterzentren für $P$}
	\BlankLine
	
	% Algo
	Wähle $c_1$ zufällig gleichverteilt aus $P$\;
	\For{$i \leftarrow 1$ \KwTo $k$}{
		Wähle den Punkt $x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$ als Zentrum $c_i$ mit Wahrscheinlichkeit $\frac{D(x')^2}{\sum_{x \in P} D(x)}$\;
	}
	Führe Lloyds Algorithmus mit den initialen Clusterzentren $c_1, \dots, c_k$ aus.
\end{algorithm}
Die $k$ Zentren, die von \kmpp{} ausgewählt werden, sind eine $\BigO{\log k}$-Approximation für das $k$-means-Problem, die durch
die anschließende Ausführung von Lloyds Algorithmus noch zu einem lokalen Optimum verbessert werden.
\absatz
Im nächsten Abschnitt betrachten wir eine konkrete Anwendung partitionierender Clusteringverfahren.

\subsection{Graphen und Clusteranalyse von Graphen}
\label{subsection:basics:graphs}

Gerade im Umgang mit großen Datenmengen hat sich in der Informatik der \emph{Graph} als geeignete Datenstruktur erwiesen. Er
ist wie folgt definiert.

\begin{definition}[Graph]
\label{def:graph}
	Sei $V$ eine endliche Menge und $E \subseteq \{ \{ u,v \} \mid u, v \in V, u \neq v \}$.
	Dann heißt das Tupel $G = (V, E)$ ein (endlicher) \emph{Graph} mit \emph{Knotenmenge} $V$ und
	\emph{Kantenmenge} $E$. Ist $e = \{ u,v \} \in E$, dann sagen wir, dass die Kante $e$ des Graphen $G$
	die Knoten $u$ und $v$ \emph{verbindet}. In diesem Fall sind $u$ und $v$ die \emph{Endknoten}
	von $e$.
	Ein Knoten $u \in V$ und eine Kante $e \in V$ heißen \emph{inzident}
	genau dann, wenn $u \in e$. Wir sagen, dass $u$ und ein weiterer Knoten $v \in V$ \emph{adjazent}
	sind genau dann, wenn es eine Kante $e' = \{ u,v \}$ in $E$ gibt.
	Typischerweise bezeichnet $n = \left|V\right|$ die \emph{Knotenzahl} von $G$ und
	$m = \left|E\right|$ die \emph{Kantenzahl} von $G$.
	
	Bei einem 3-Tupel $G' = (V', E', w')$ mit $w' : E \rightarrow \mathbb{N}$ spricht man von einem \emph{gewichteten} Graphen,
	dessen Kanten über ein Gewicht verfügen, das von der Gewichtsfunktion $w'$ abgebildet wird.
\end{definition}
Für die konkrete Datenhaltung von Graphen haben sich im Wesentlichen zwei Ansätze als praktikabel erwiesen: Bei den sogenannten
\emph{Adjazenzlisten} wird für jeden Knoten $v$ im Graphen eine Liste $Adj_v$ gehalten, in der für jeden zu $v$ inzidenten
Knoten ein Eintrag in $Adj_v$ enthalten ist, der den entsprechenden adjazenten Knoten referenziert, sowie gegebenenfalls das
Gewicht der Kante zwischen den beiden Knoten.
Alternativ wird in einer
\emph{Adjazenzmatrix} $Adj^G$ der Größe $\left|V\right| \times \left|V\right|$ an der Stelle $Adj^G_{u,v}$ das Gewicht der Kante
zwischen den Knoten $u$ und $v$ eingetragen, sofern die beiden Knoten durch eine Kante miteinander verbunden sind.
Anderenfalls wird zumeist $-1$ oder $0$ eingetragen. Bei ungewichteten Graphen wird dementsprechend lediglich $0$ oder $1$
eingetragen.
\absatz
Wenn die Eingabe keine Punktmenge, sondern ein (gewichteter) Graph ist, können wir die Ideen partitionierender Clusteringverfahren
übertragen. Wir interessieren uns in diesem Fall für die "`Ähnlichkeit"' von Knotenmengen im Graphen.

\begin{definition}[Graph-Schnitt]
	Sei $G=(V,E,w)$ ein gewichteter Graph. Ein \emph{Schnitt} $C=(S,T)$ von $G$ ist eine Partitionierung von $V$ in die beiden
	Mengen $S$ und $T$, das heißt, dass $V = S \cup T$. Die \emph{Schnittmenge} von $C$ sind die Kanten in $E$, die einen
	Endpunkt in $S$ und den anderen Endpunkt in $T$ haben, das heißt formal ist die Schnittmenge definiert als
	\[ \{ (u,v) \in E \mid u \in S, v \in T \}. \]
	Das Gewicht oder der Wert eines Schnittes ist die Summe der Kantengewichte der Schnittmenge. Wir verwenden die folgende
	Notation:
	\[ w_{cut}(S,T) = \sum_{u \in S, v \in T} w((u,v)) \]
	Falls der Graph in Form einer Adjazenzmatrix $Adj^G$ vorliegt, lautet die Berechnungsvorschrift entsprechend:
	\[ w_{cut}(S,T) = \sum_{u \in S, v \in T} Adj^G_{u,v} \]
\end{definition}
Für eine direkte Analogie zum $k$-means-Problem wäre ein Schnitt- beziehungsweise Partitionierungs-Begriff wünschenswert,
der eine $k$-fache Partitionierung der Knotenmenge erlaubt. Diese lautet wie folgt.

\begin{definition}[$k$-Graphpartitionierung]
	Sei $G=(V,E,w)$ ein gewichteter Graph. Für zwei Mengen $A, B \subseteq V$ definieren wir:
	\[ w(A,B) = \sum_{u \in A, v \in B} w((u,v)) \]
	Das $k$-Graphpartitionierungsproblem besteht darin, eine Partitionierung der Knotenmenge $V$ in $k$ disjunkte Teilmengen
	$V_1, \dots, V_k$ mit $\bigcup_{i \in \{1, \dots, k \}} V_i = V$ zu ermitteln, sodass sich die Knoten innerhalb einer
	Partition bezüglich einer Ähnlichkeitsrelation möglichst ähnlich sind und die Knoten unterschiedlicher Partitionen
	bezüglich der Ähnlichkeitsrelation möglichst stark voneinander unterscheiden.
	
	Beim Clustering von Punktmengen lagen für die Ähnlichkeitsrelation Metriken nahe, im Falle der Graphpartitionierung
	existiert eine Reihe von Optimierungskriterien, von denen wir im Folgenden die verbreitetsten einführen.
	
	\begin{enumerate}
		\item 	\textbf{Ratio Association}. Bei der Ratio Association sollen die Intra-Clusterabstände relativ zur jeweiligen
				Clustergröße maximiert werden:
				\[ \max_{V_1, \dots, V_k} \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|} \]
		\item 	\textbf{Ratio Cut}. Beim Ratio Cut wird der Schnitt zwischen jeweils einem Cluster und allen anderen Punkten
				im Graphen minimiert:
				\[ \min_{V_1, \dots, V_k} \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{\left|V_c\right|} \]
		\item 	\textbf{Kernighan-Lin}. Bei dem in~\cite{KernighanL70} vorgestellten Optimierungskriterium werden die
				Intra-Clusterabstände ähnlich der Ratio Association minimiert, allerdings müssen alle Partition hier zusätzlich
				die selbe Größe haben. Die hier vorgestellte Zielfunktion ist von $2$ auf $k$ Partitionen verallgemeinert:
				\[ \min_{V_1, \dots, V_k} \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|} \textrm{ s.t. } 
				   \left|V_c\right| = \frac{\left|V\right|}{k} \, \forall \, c \in \{ 1, \dots, k \} \]
		\item 	\textbf{Normalized Cut}. Ziel des Normalized Cut ist wie beim Ratio Cut die Minimierung des Schnitts von einem
				Cluster mit den übrigen Punkten im Graphen, jedoch relativ zum Schnitt des Clusters mit \emph{allen}
				Knoten im Graphen. Der Normalized Cut oder kurz NCut ist in der Bild-Segmentierung recht verbreitet~\cite{ShiM00}.
				Wir betrachten hier ebenfalls die auf $k$ Partitionen verallgemeinerte Zielfunktion.
				\[ \min_{V_1, \dots, V_k} \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{w(V_c, V)} \]
	\end{enumerate}
\end{definition}
Im nächsten Abschnitt betrachten wir zwei Techniken, mit denen Limitierungen hinsichtlich der Qualität der erzielten Clusterings
sowie der Komplexität der Berechnungen verbessert werden können.

\subsection{Kernel-Methoden und spektrales Clustering}
\label{subsection:basics:kernel-spectral}

Legt man die $k$-means Zielfunktion zu Grunde, können im Ursprungsraum $\Rd$ nur Cluster berechnet werden, die \emph{linear
separierbar} sind. Ein illustratives Beispiel im $\mathbb{R}^2$ sind zwei konzentrisch angeordnete ringförmige Punktmengen.
Die gemäß der $k$-means Zielfunktion optimalen Cluster sind die beiden jeweiligen Ringe, es ist jedoch im $\mathbb{R}^2$ nicht
möglich Clusterzentren anzugeben, die den zweidimensionalen Raum in Ringe partitionieren. Die Beschränkung liegt darin, dass
wir die Partitionierung oder Separierung durch Hyperebenen vornehmen. Im Ursprungsraum können wir daher nur linear separierbare
Cluster bestimmen. Wir betrachten nun ein Verfahren, mit dem sich diese Beschränkung umgehen lässt.

\subsubsection{Kernel-\texorpdfstring{$k$}{k}-means}

Unsere fundamentale Motivation für diesen Abschnitt ist Covers Theorem~\cite{Cover65}, welches informell zusammengefasst besagt,
dass eine zufällige Eingabepunktmenge im $d$-dimensionalen Raum mit hoher Wahrscheinlichkeit in einen höherdimensionalen
Raum (zum Beispiel der Dimension $D \gg d$) abgebildet linear separierbar ist.

Für Klassifizierungs- und insbesondere Clusteringprobleme benötigen wir sowohl im Ursprungsraum als auch im abgebildeten Raum
ein algebraisches Maß für Ähnlichkeit. Üblicherweise wird dabei das \emph{Skalarprodukt} eingesetzt. Dabei beziehen wir uns
im Weiteren auf die folgende Definition von \emph{euklidischen Vektorräumen}, also Vektorräumen mit einem reellen
Skalarprodukt.

\begin{definition}[Euklidischer Vektorraum]
\label{def:innerproductspace}
	Sei $V$ ein $\mathbb{R}$-Vektorraum. Ein reelles Skalarprodukt auf $V$ ist eine Abbildung
	$\innerproduct{\cdot}{\cdot} : V \times V \rightarrow \mathbb{R}$, die den folgenden Eigenschaften genügt:
	\begin{enumerate}
		\item 	$\innerproduct{a}{b} = \innerproduct{b}{a} \, \forall \, a, b \in V$
		\item 	$\innerproduct{\lambda a + \mu b}{c} = \lambda \innerproduct{a}{c} + \mu \innerproduct{b}{c} \,
				\forall \, a,b,c \in V \textrm{ und } \lambda, \mu \in \mathbb{R}$
		\item 	$\innerproduct{a}{a} \geq 0 \textrm{ und } \innerproduct{a}{a} = 0 \Leftrightarrow a = 0 \, \forall \, a \in V$
	\end{enumerate}
	Ein $\mathbb{R}$-Vektorraum mit einem reellen Skalarprodukt heißt \emph{euklidischer Vektorraum}.
\end{definition}
Die $k$-means-Zielfunktion kann auf euklidische Vektorräume erweitert werden, indem wir für einen euklidischen Vektorraum
$\mathcal{V}$ eine Norm $\lVert \cdot \rVert : \mathcal{V} \rightarrow \mathbb{R}$ über das Skalarprodukt gemäß der folgenden
Abbildungsvorschrift für ein $\mathpzc{x} \in \mathcal{V}$ definieren:
$\lVert \mathpzc{x} \rVert = \sqrt{\innerproduct{\mathpzc{x}}{\mathpzc{x}}}$.
Das auf Skalarprodukten basierte Analog zu Definition~\ref{def:kmeans-kmedian} des $k$-means-Problems lautet dann wie folgt:

\begin{definition}[$k$-means in euklidischen Vektorräumen]
\label{def:kmeans-kmedian-innerproduct}
Sei $\mathcal{V}$ ein euklidischer Vektorraum mit der Norm $\lVert \cdot \rVert : \mathcal{V} \rightarrow \mathbb{R}$
und sei $\mathcal{P} \subset \mathcal{V}$ sowie $k \in \mathbb{N}^{+}$.
Das $k$-means-Problem in $\mathcal{V}$ besteht darin, eine Menge von $k$ (Cluster-)Zentren
$\mathcal{C} = \{ \mathpzc{c}_1, \dots, \mathpzc{c}_k \}$ mit $\mathpzc{c}_i \in V$ zu finden, sodass der folgende Term
minimal wird:
\[ \sum_{\mathpzc{p} \in \mathcal{P}} \min_{\mathpzc{c} \in \mathcal{C}} \EuclidSquared{\mathpzc{p}}{\mathpzc{c}} \]
Die gewichtete Variante des $k$-means-Problems in euklidischen Vektorräumen lautet entsprechend wie folgt:
\[ \sum_{\mathpzc{p} \in \mathcal{P}} \min_{\mathpzc{c} \in \mathcal{C}} w(\mathpzc{p})
	\EuclidSquared{\mathpzc{p}}{\mathpzc{c}} \]
\end{definition}
Damit ist es nun möglich, ein intuitives Verfahren zum Bestimmen von Clusterings mit nicht-linearen Separatoren anzugeben.
Wir bilden zunächst mit einer Abbildung $\phi : \mathcal{X} \rightarrow \mathcal{V}$ unsere Eingabepunkte aus ihrem Ursprungsraum
$\mathcal{X}$ (also beispielsweise $\Rd$) auf einen $D$-dimensionalen (üblicherweise $D \gg d$) euklidischen
Vektorraum $\mathcal{V}$ ab, und berechnen in diesem mit Lloyds Algorithmus oder einem anderen Clusteringverfahren ein
Clustering, welches im Ursprungsraum $\mathcal{X}$ im Allgemeinen nicht-linear separierten Clustern entspricht.
Bei diesem Verfahren wird entsprechend der folgende Term minimiert:
\[ \sum_{p \in P} \min_{c \in C} \EuclidSquared{\phi(p)}{c} \]
Ein Clusterzentrum $c_i$ ist dementsprechend definiert durch
\[ c_i = \frac{\sum_{p \in S_i} \phi(p)}{\left|S_i\right|} \]
wobei $S_i$ die Menge aller dem Cluster $c_i$ zugeordneten Punkte ist. Zunächst haben wir uns damit nicht-linear separierte
Cluster "`erkauft"', der Preis, den wir dafür zahlen, besteht jedoch in höherem Rechenaufwand, da wir einerseits die Punkte
in den jeweils höherdimensionalen Raum abbilden müssen und andererseits die Berechnung der (euklidischen) Distanzen aufwändiger
wird; diese benötigt im Ursprungsraum Zeit $\mathcal{O}(d)$ und im höherdimensionalen Raum $\mathcal{O}(D)$. Der Mehraufwand
für die Distanzberechnung fällt dabei stärker ins Gewicht, da wir die Distanzberechnung beispielsweise bei Lloyds Algorithmus
in jeder Schleifeniteration durchführen.
\absatz
Dieses Problem wurde erstmals im Zusammenhang mit \emph{Support Vector Machines} (kurz SVMs) gelöst~\cite{BoserGV92}.
Wir übertragen die Lösung mittels des sogenannten \emph{Kernel-Tricks} in den Clustering-Kontext und skizzieren diese kurz.
Zunächst halten wir eine wichtige Beobachtung hinsichtlich der (quadrierten) euklidischen Distanz $\EuclidSquared{\phi(p)}{c}$
zwischen dem abgebildeten Punkt und einem Clusterzentrum fest. Diese lässt sich wie folgt ausschließlich über Skalarprodukte
berechnen (wobei auch hier wieder $S_c$ die dem Clusterzentrum $c$ zugeordnete Punktmenge ist):
\begin{align}
\label{formula:kmeans-innerproduct}
	\EuclidSquared{\phi(p)}{c} &= \bigg\lVert \phi(p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \phi(x) \bigg\rVert \notag \\
	&= \bigg\langle \phi(p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \phi(x),
		\phi(p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \phi(x) \bigg\rangle \notag \\
	&= \innerproduct{\phi(p)}{\phi(p)} - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \innerproduct{\phi(p)}{\phi(x)}
		- \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \innerproduct{\phi(x)}{\phi(p)} \notag \\
		& \hspace{0.4cm} + \frac{1}{\left|S_c\right|^2} \sum_{x,y \in S_c} \innerproduct{\phi(x)}{\phi(y)}
\end{align}
Diese Beobachtung allein hilft uns noch nicht weiter, da auch die Berechnung des Skalarproduktes im höherdimensionalen
Raum einen asymptotischen Aufwand von $\mathcal{O}(D)$ hätte. An dieser Stelle kommt nun der in~\cite{BoserGV92} vorgestellte
\emph{Kernel-Trick} zum Tragen. Durch den Einsatz von sogenannten \emph{Kernelfunktionen} lässt sich dieser Mehraufwand
umgehen. Dabei handelt es sich um positiv definite Abbildungen $\kappa : X \times X \rightarrow \mathbb{R}$, die bei Eingabe von
zwei Punkten aus dem Ursprungsraum $X$ das Skalarprodukt der in den höherdimensionalen Raum abgebildeten Punkte berechnen:
\[ \kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)} \]
Der Kernel-Trick besteht also darin, eine Kernelfunktion zur Berechnung der Skalarprodukte im höherdimensionalen Raum zu
verwenden, für die keine explizite Kenntnis der Abbildung $\phi$ oder der abgebildeten Punkte $\phi(p)$ nötig ist.
Mit der Kernelfunktion $\kappa$ können wir nun die Distanzberechnung aus ~\ref{formula:kmeans-innerproduct} weiter vereinfachen:
\begin{align}
\label{formula:kernel-kmeans}
	\EuclidSquared{\phi(p)}{c} = \kappa(p,p) - \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \kappa(p, x)
		- \frac{1}{\left|S_c\right|} \sum_{x \in S_c} \kappa(x, p)
		+ \frac{1}{\left|S_c\right|^2} \sum_{x,y \in S_c} \kappa(x, y)
\end{align}

\begin{table}[t]
\centering
\begin{tabular}{@{}lr@{}} \toprule
	\textbf{Linear-Kernel} & $\kappa(x_i, x_j) = x_i \cdot x_j + \gamma $ \\
	\textbf{Polynom-Kernel} & $\kappa(x_i, x_j) = \left( x_i \cdot x_j + \gamma \right)^\delta$ \\
	\textbf{Gauss-/RBF-Kernel} & $\kappa(x_i, x_j) = \exp \left( - \frac{\EuclidSquared{x_i}{x_j}}{2 \sigma^2} \right)$ \\
	\textbf{Sigmoid-Kernel} & $\kappa(x_i, x_j) = \tanh(\alpha(x_i \cdot x_j) + \theta) $ \\ \bottomrule
\end{tabular}
\caption{Beispiele für häufig eingesetzte Kernelfunktionen.}
\label{tbl:kernel-functions}
\end{table}
Tabelle~\ref{tbl:kernel-functions} bietet eine Übersicht über die am häufigsten eingesetzten Kernelfunktionen. Insbesondere
die Gauss-Kernelfunktion und die Polynom-Kernelfunktion werden auch bei Support-Vektor-Machines gerne verwendet.
\absatz
Eine auf Kernelfunktionen basierende Variante von (Lloyds) Algorithmus~\ref{algo:lloyd} folgt mit \ref{formula:kernel-kmeans}
unmittelbar. Der Algorithmus wird üblicherweise kurz "`Kernel-$k$-means-Algorithmus"' genannt.

\begin{algorithm}[H]
\label{algo:kernel-lloyd}
\caption{Kernel-$k$-means}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}, \kappa : X \times X \rightarrow \mathbb{R}$}
	\Output{$k$-means-Clustering von $P$}
	\BlankLine
	
	% Algo
	Wähle zufällig $k$ Zentren $c_1^{(0)}, \dots, c_k^{(0)}$ aus $P$ oder $\Rd$\;
	$t \leftarrow 0$\;
	\Repeat{\textnormal{Konvergenz oder} $t > t_{max}$}{
		Berechne $\EuclidSquared{\phi(p)}{c_i^{(t)}}$ für alle $p \in P$ und $i \in \{1, \dots, k\}$ mit~\ref{formula:kernel-kmeans}\;
		\ForEach{$p \in P$}{
			$c^*(p) \leftarrow \argmin_i\left( \bigg\lVert \phi(p) - c_i^{(t)} \bigg\rVert \right)$\;
		}
		$t \leftarrow t + 1$\;
		\For{$i \leftarrow 1$ \KwTo $k$}{
			$S_i^{(t)} \leftarrow \{ p \mid c^*(p) = i \}$\;
			$c_i^{(t)} \leftarrow \frac{1}{|S_i^{(t)}|} \sum_{p_j \in S_i^{(t)}} p_j $\;
		}
	}
\end{algorithm}
Der Vorfaktor für gewichtetes $k$-means hat auf die Berechnungsvorschriften keine weiteren Auswirkungen,
Algorithmus~\ref{algo:kernel-lloyd} kann dementsprechend für gewichtetes $k$-means erweitert werden. Der Algorithmus
lässt sich mit einer asymptotischen Laufzeit von $\mathcal{O}(n^2 (\tau + d))$ implementieren~\cite{DhillonGK04}, wobei
$\tau$ die Anzahl an Iterationen der äußeren Schleife ist.
\absatz
Im nächsten Unterabschnitt führen wir Verfahren ein, die mit Techniken aus der linearen Algebra das Graphclusteringproblem
lösen und in Kombination mit dem hier vorgestellten Algorithmus eine robuste Grundlage für unsere Zwecke bilden.

\subsubsection{Spektrale Clustering Methoden}