\section{Kernel-\texorpdfstring{$k$}{k}-means Methoden für spektrales Graphclustering}
\label{section:main}

In diesem Kapitel wollen wir Verfahren zur spektralen Clusteranalyse von Graphen vorstellen und verbessern. Als Grundlage wollen
wir den Kernel-$k$-means Algorithmus verwenden. Lloyds Algorithmus ist nach wie vor eine Heuristik, die in der Praxis mit geringen
Ausführungszeiten Clusterings von akzeptabler Qualität berechnet. Unter Einsatz der Kernel Methode können wir die
Qualität der berechneten Clusterings noch einmal steigern, ohne dabei signifikante Performanzeinbußen in Kauf nehmen zu müssen.
Der Algorithmus ist zudem leicht zu implementieren. Gerade im Bereich der Clusteranalyse sollte dies nicht unterschätzt werden,
da eine praktische Evaluation der Algorithmen in diesem anwendungsreichen Gebiet unerlässlich ist.
\absatz
Das Kapitel ist folgendermaßen gegliedert: In Abschnitt~\ref{subsection:wkkm-graphcut-graphclustering} diskutieren wir, wie wir
mit dem Kernel-$k$-means Algorithmus Graphclusteringprobleme (und umgekehrt) lösen können. In Abschnitt~\ref{subsection:kernelkmpp}
zeigen wir, wie sich die Kernel Methode auch auf den Algorithmus \kmpp{} anwenden lässt und wie wir damit den
Kernel-$k$-means Algorithmus für Graphclustering verbessern können.

\subsection{Graphschnitte und Graphclustering mit gewichtetem Kernel-\texorpdfstring{$k$}{k}-means}
\label{subsection:wkkm-graphcut-graphclustering}

Um die Graphclusteringprobleme aus Definition~\ref{def:graphpartitioning} mit dem Kernel-$k$-means Algorithmus zu lösen,
müssen wir uns zunächst überlegen, wie wir diese in eine Eingabe für den Algorithmus transformieren können. Dass eine solche
Transformation (bidirektional) möglich ist, haben bereits Dhillon, Guan und Kulis gezeigt~\cite{DhillonGK04,DhillonGK07}. Wir
fassen daher zunächst den für uns relevanten Teil ihrer Ergebnisse zusammen.
\absatz
Die grundlegende Idee besteht darin, zu zeigen, dass sowohl die Zielfunktion für Kernel-$k$-means als auch die (diversen)
Zielfunktionen für Graphpartitionierungsprobleme als \emph{Spurmaximierungsprobleme} formuliert werden können. Die
\emph{Spur} (engl. \emph{trace}) einer quadratischen $n \times n$-Matrix $M$ ist dabei die Summe der Elemente auf der
Hauptdiagonalen von $M$:
\[ \tr{M} = \sum_{i=1}^{n} M_{i,i} \]
Weiterhin gilt für zwei quadratische Matrizen $M,M'$:
\begin{enumerate}
	\item 	$\tr{MM'} = \tr{M'M}$
	\item 	$\tr{M^TM} = \frobenius{M}^2$
	\item 	$\tr{M+M'} = \tr{M} + \tr{M'}$
\end{enumerate}
Wir formen zunächst die Kernel-$k$-means Zielfunktion um und widmen uns dann den Graphpartitionierungsproblemen.

\paragraph{Gewichtetes Kernel-$k$-means.} Für die Umformung des $k$-means-Problems legen wir das \emph{gewichtete}
Kernel-$k$-means Problem (engl. \emph{weighted kernel $k$-means}, kurz WKKM) zu Grunde.
Dieses lautet analog zu Definition~\ref{def:kmeans-kmedian-innerproduct} wie folgt.
Unsere Eingabe besteht aus der gewichteten Punktmenge
als $n \times d$-Matrix $A$, einer Gewichtsfunktion $w$, die einen Punkt-Zeilenvektor $\vect{p}$ aus $A$ auf ein reelles Gewicht
abbildet, und der Clusteranzahl $k$. Die Zielfunktion besteht wie gewohnt darin, die quadrierten euklidischen Distanzen der
Punkte eines Clusters zu ihrem Clusterzentrum zu minimieren:
\begin{align}
\label{formula:wkkm-objective}
	\min D(\{S_i\}_{i=1}^{k}) = \sum_{i=1}^{k} \sum_{\vect{p} \in S_i} w(\vect{p}) \EuclidSquared{\phi(\vect{p})}{\vect{c}_i}
\end{align}
Dabei sind die $S_1, \dots, S_k$ die Cluster, wobei ein Cluster $S_i$ das Zentrum
\[ \vect{c}_i = \frac{\sum_{\vect{p} \in S_i} w(\vect{p}) \phi(\vect{p})}{\sum_{\vect{p} \in S_i} w(\vect{p})} \]
hat. Wir setzen auch hier wieder den Kernel-Trick für die Distanzberechnungen ein. Dazu berechnen wir initial $n^2$ viele
Skalarprodukte $\innerproduct{\phi(\vect{p}_i)}{\phi(\vect{p}_j)}$ und speichern diese in einer
$n \times n$-Kernelmatrix $K$ mit $K_{i,j} = \kappa(\vect{p}_i, \vect{p}_j) = \innerproduct{\phi(\vect{p}_i)}{\phi(\vect{p}_j)}$.
Die Distanzberechnung von einem Punkt zu seinem Clusterzentrum
können wir ebenfalls analog zu~\ref{formula:kernel-kmeans} mit Hilfe der Kernelmatrix umformulieren. Sie unterscheidet sich von
dieser nur darin, dass die Punkte gewichtet sind. Mit $\left| S_i \right| = \sum_{\vect{p}_j \in S_i} w(\vect{p}_j)$ gilt:
\begin{align}
\label{formula:wkkm}
	\EuclidSquared{\phi(\vect{p}_j)}{\vect{c}_i} =
	K_{i,i} - \frac{2 \sum_{\vect{p}_j \in S_i} w(\vect{p}_j) K_{i,j}}{\sum_{\vect{p}_j \in S_i} w(\vect{p}_j)}
	+ \frac{\sum_{\vect{p}_j, \vect{p}_l \in S_i} w(\vect{p}_j) w(\vect{p}_l) K_{j,l}}{\left(\sum_{\vect{p}_j \in S_i} w(\vect{p}_j)\right)^2}
\end{align}
Die Erweiterung von Algorithmus~\ref{algo:kernel-lloyd} zu Algorithmus~\ref{algo:wkkm} ergibt sich ebenfalls unmittelbar.
\begin{algorithm}
\caption{Weighted Kernel-$k$-means}
\label{algo:wkkm}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	\SetKw{KwGoTo}{go to}
	
	% Input/Output
	\Input{Kernel-Matrix $K$, $k \in \mathbb{N}^{+}$, Punktgewichte $w$}
	\Output{$k$-means-Clustering der Eingabepunktmenge}
	\BlankLine
	
	% Algo
	Wähle $k$ initiale Cluster $S_1^{(0)}, \dots, S_k^{(0)}$;
	$t \leftarrow 0$\;
	\ForEach{$\vect{p}$}{\label{iter}
		Bestimme $c^*(\vect{p}) \leftarrow \argmin_{i=1}^{k} \EuclidSquared{\phi(\vect{p})}{\vect{c_i}}$ mit~\ref{formula:wkkm}\;
	}
	\For{$i \leftarrow 1$ \KwTo $k$}{
		$S_i^{(t+1)} \leftarrow \{ \vect{p} : c^*(\vect{p}) = i \}$\;
	}
	$t \leftarrow t+1$\;
	\If{$t < t_{max}$ \textbf{and} nicht konvergiert}{
		\KwGoTo \ref{iter}\;
	}\Else{
		\Return{$S_1^{(t)}, \dots, S_k^{(t)}$}
	}
\end{algorithm}
\paragraph{Gewichtetes Kernel-$k$-means als Spurmaximierung.}
Wir formen als nächstes die Zielfunktion des gewichteten Kernel-$k$-means Problems in ein Spurmaximierungsproblem um.
Dazu gehen wir zunächst davon aus, dass die Punktgewichte in einer $n \times n$-Diagonalmatrix $W$ gespeichert sind, sodass
$W_{j,j} = w(\vect{p}_j)$ für alle Punkte $\vect{p}_j$ und die Punktgewichte des Clusters $S_i$ in der
$\left|S_i\right| \times \left|S_i\right|$-Diagonalmatrix
$W^i$ mit $W_{m,m}^j = w(\vect{p}_m)$ für alle Punkte $\vect{p}_m \in S_i$ gespeichert sind. 
\absatz
Sei nun $s_i$ die Summe der Gewichte der Punkte in Cluster $i$, also
\[ s_i = \sum_{\vect{p}_j \in S_i} w(\vect{p}_j) = \sum_{\vect{p}_j \in S_i} W_{j,j}^i \]
Wir betrachten die $n \times k$-Matrix $Z$ mit den Einträgen
\[ Z_{j, i} = 	\begin{cases}
					\frac{1}{\sqrt{s_i}}, & \textrm{ falls } \vect{p}_j \in S_i, \\
					0 & \textrm{ sonst }
				\end{cases} \]
sowie die (nicht explizit berechnete) Matrix $\Phi_i = [\phi(\vect{p}_1), \dots, \phi(\vect{p}_m)]$ von abgebildeten Punkten
des Clusters $S_i = \{ \vect{p}_1, \dots, \vect{p}_m \}$. Mit diesen können wir
die Zielfunktion des gewichteten Kernel-$k$-means Problems~(\ref{formula:wkkm-objective}) umformen. Wir erinnern uns, dass diese
die Minimierung der Summe der Intraclusterabstände ist. Wir schreiben für den Intraclusterabstand eines Clusters $S_i$:
$d(S_i) = \sum_{\vect{p} \in S_i} w(\vect{p}) \EuclidSquared{\phi(\vect{p})}{\vect{c}_i}$. Damit können wir den Zielfunktionswert
$D(\{S_i\}_{i=1}^{k})$ folgendermaßen vereinfachen:
\[ D(\{S_i\}_{i=1}^{k}) = \sum_{i=1}^k d(S_i) \]
Der Zentroidvektor $c_i$ des Clusters $S_i$ berechnet sich in unserer ursprünglichen Problemformulierung durch
\[ \vect{c}_i = \frac{\sum_{\vect{p} \in S_i} w(\vect{p}) \phi(\vect{p})}{\sum_{\vect{p} \in S_i} w(\vect{p})} \]
Diese Berechnungsvorschrift ist wegen der Definition von $\Phi_i$ äquivalent zu
\[ \vect{c}_i = \Phi_i \frac{W^j \vect{e}}{s_j}, \]
wobei $\vect{e}$ der Einsvektor der passenden Dimension ist.
Wir können alternativ unter Einsatz der Matrix $Z$ auch direkt eine Matrix $C$ aller Clusterzentren bestimmen,
wenn wir für die gesamte Eingabepunktmenge die (ebenfalls nicht explizit berechnete) Abbildungsmatrix
$\Phi = [\Phi_1, \dots, \Phi_k] = [\phi(\vect{p}_1), \dots, \phi(\vect{p}_n)]$ verwenden:
\[ C = \Phi W Z Z^T \]
In diesem Fall entspricht das Zentrum eines einzelnen Clusters $S_i$
\[ \vect{c}_i = \left(\Phi W Z Z^T\right)_{\cdot i} \]
wobei wir für eine Matrix $M$ mit $M_{\cdot i}$ die $i$-te Spalte von $M$ meinen. Damit ist es uns möglich, den
Zielfunktionswert so zu formulieren, dass wir später eine Äquivalenz zur Graphpartitionierung sehen werden. Mit
%$Y = W^{\frac{1}{2}} Z$ gilt:%
$Y = W^{\frac{1}{2}} Z = U D^{\frac{1}{2}} U^{-1} Z$ gilt:
\begin{align*}
	D(\{S_i\}_{i=1}^{k}) &= \sum_{i=1}^{k} \sum_{\vect{p} \in S_i} w(\vect{p}) \EuclidSquared{\phi(\vect{p})}{\vect{c}_i} \\
		&= \sum_{j=1}^{n} W_{j,j} \bigg\lVert \Phi_{\cdot j} - \left( \Phi W Z Z^T \right)_{\cdot j} \bigg\rVert^2 \\
		&= \sum_{j=1}^{n} W_{j,j} \bigg\lVert \Phi_{\cdot j} - \left( \Phi W^{\frac{1}{2}} Y Y^T W^{-\frac{1}{2}} \right)_{\cdot j} \bigg\rVert^2 \\
		&= \sum_{j=1}^{n} \bigg\lVert \Phi_{\cdot j} \left(W_{j,j}\right)^{\frac{1}{2}} - \left( \Phi W^{\frac{1}{2}} Y Y^T \right)_{\cdot j} \bigg\rVert^2 \\
		&= \big\lVert \Phi W^{\frac{1}{2}} - \Phi W^{\frac{1}{2}} Y Y^T \big\rVert_F^2
\end{align*}
Der ursprüngliche Zielfunktionswert ist damit auf die (quadrierte) Frobenius-Norm von
$\Phi W^{\frac{1}{2}} - \Phi W^{\frac{1}{2}} Y Y^T$ transformiert.
Wir können nun die oben genannte Verbindung zwischen der Frobenius-Norm und der Matrix-Spur ausnutzen, um zur beabsichtigten
Spur-Maximierung zu gelangen. Wir erinnern uns, dass für eine Matrix $M$ gilt, dass $\tr{M^TM} = \frobenius{M}^2$. Wir können
somit weiter umformen:
\begin{align*}
	D(\{S_i\}_{i=1}^{k}) &= \tr{ W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} - W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} Y Y^T \\
		& \; \; \; \; - Y Y^T W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} + Y Y^T W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} Y Y^T } \\
		&= \tr{ W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} } - \tr{ Y^T W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} Y }
\end{align*}
In der Kernel-Matrix $K$ speichern wir sämtliche Skalarprodukte der projizierten Punkte, demnach ist per Definition
$K = \Phi^T \Phi$.
Die Kernel-Matrix ist genau wie die Gewichts-Matrix $W$ unabhängig von der Lösung. Dementsprechend ist auch
$\tr{ W^{\frac{1}{2}} K W^{\frac{1}{2}} }$ unabhängig von der Lösung. Die Minimierungszielfunktion~\ref{formula:wkkm-objective} ist
daher gleichzusetzen mit der Maximierungszielfunktion des folgenden Spurmaximierungsproblems:
\begin{align}
\label{formula:wkkm-tracemax}
	\max_{Y}{} \tr{ Y^T W^{\frac{1}{2}} K W^{\frac{1}{2}} Y }
\end{align}
Die Spur der Matrix $Y$ hat offensichtlich hat offensichtlich einen wichtigen Anteil an der Qualität der Lösung. Wir wollen daher
abschließend noch kurz einen genaueren Blick auf $Y$ werfen.
Die Matrix $Y = W^{\frac{1}{2}} Z$ ist eine orthonormale Diagonalmatrix, das heißt $Y^T Y = I$. Ihre Einträge sehen
wie folgt aus:
\[ Y = 	\begin{pmatrix}
			\frac{W_1^{\frac{1}{2}} \vect{e}}{\sqrt{s_1}} & & & \\
			& \frac{W_2^{\frac{1}{2}} \vect{e}}{\sqrt{s_2}} & & \\
			& & \dots & \\
			& & & \frac{W_k^{\frac{1}{2}} \vect{e}}{\sqrt{s_k}}
		\end{pmatrix} \]

\paragraph{Graphpartitionierung als Spurmaximierung.}
Es ist möglich, für alle in Definition~\ref{def:graphpartitioning} vorgestellten Problemstellungen ein äquivalentes
Spurmaximierungsproblem anzugeben. Wir beschränken uns hier zunächst auf die beiden Zielfunktionen mit praktischer Relevanz
für das Evaluations-Kapitel dieser Arbeit, nämlich die Ratio Association und den Normalized Cut. Wir beginnen mit der
Ratio As\-so\-ci\-a\-tion. Das hier verwendete $w$ entspricht dem aus Definition~\ref{def:graphpartitioning}.
Wir führen zunächst einen Indikator-Vektor $\vect{x}^c$ der Dimension $n$ für jede Partition beziehungsweise jedes Cluster $c$
ein, sodass $\vect{x}^c(i) = 1$ genau dann, wenn das Cluster $c$ den Knoten $i$ enthält. Die Zielfunktion bei der Ratio
Association lautet
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|} \]
Wir nehmen an, dass $A$ die (gewichtete) Adjazenzmatrix des Eingabegraphen ist und definieren zusätzlich
\[ \tilde{\vect{x}}^c = \frac{\vect{x}^c}{((\vect{x}^c)^T \vect{x}^c)^\frac{1}{2}} \]
Das Produkt $(\vect{x}^c)^T \vect{x}^c$ entspricht der Größe der Partition $c$:
\[ \left|V_c\right| = (\vect{x}^c)^T \vect{x}^c \]
Die Summe der Kantengewichte der Partition $w(V_c, V_c)$ können wir über folgendes Produkt abbilden:
\[ w(V_c, V_c) = (\vect{x}^c)^T A \vect{x}^c \]
Damit können wir eine erste Transformation der Zielfunktion vornehmen:
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|}
	\Leftrightarrow \max \sum_{c=1}^{k} \frac{(\vect{x}^c)^T A \vect{x}^c}{(\vect{x}^c)^T \vect{x}^c}
	\Leftrightarrow \max \sum_{c=1}^{k} (\tilde{\vect{x}}^c)^T A \tilde{\vect{x}}^c \]
Betrachten wir $\tilde{X}$ als die Matrix, deren $c$-te Spalte der Vektor $\tilde{\vect{x}}^c$ ist, können wir die Zielfunktion
umformen zu
\begin{align}
\label{formula:ratioassoc-tracemax}
	\max_{\tilde{X}}{} \tr{ \tilde{X}^T A \tilde{X} }
\end{align}
Wenn wir in~\ref{formula:wkkm-tracemax} für die Gewichtsmatrix $W$ die Einheitsmatrix wählen, also $W = \mathbbm{1}_n$ und
die Adjazenzmatrix als Kernel-Matrix setzen, sodass $K = A$, dann sind~\ref{formula:ratioassoc-tracemax}
und~\ref{formula:wkkm-tracemax} äquivalent. Auf diese Weise können wir das Ratio Association Problem mit unserem Algorithmus
für gewichtetest Kernel-$k$-means lösen.
\absatz
Für den Normalized Cut gehen wir ähnlich vor. Unsere ursprüngliche Zielfunktion lautet
\[ \min \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{w(V_c, V)} \]
Zunächst wollen wir die Funktion in ein Maximierungsproblem umwandeln. Wir beobachten, dass die Zielfunktion äquivalent ist
zu
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{w(V_c, V)} \]
Wir benötigen zur Umformung des Normalized Cut die diagonale Gradmatrix $D$ des Graphen, welcher der Adjazenzmatrix $A$ zu
Grunde liegt. Wir verwenden wieder die Indikator-Vektoren und definieren zusätzlich
\[ \hat{\vect{x}}^c = \frac{\vect{x}^c}{ ((\vect{x}^c)^T D \vect{x}^c)^{\frac{1}{2}} } \]
Unter Verwendung der Gradmatrix können wir die Variante des Normalized Cut als Maximierungsproblem wie folgt umformen:
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{w(V_c, V)}
	\Leftrightarrow \max \sum_{c=1}^{k} \frac{(\vect{x}^c)^T A \vect{x}^c}{(\vect{x}^c)^T D \vect{x}^c} 
	\Leftrightarrow \max \sum_{c=1}^{k} (\hat{\vect{x}}^c)^T A \hat{\vect{x}}^c \]
Im mittleren Term zählen wir die Kanten im Zähler und Nenner jeweils doppelt. Dies gleich sich dann bei der Division aus.
Wir nehmen analog zur Ratio Association an, dass die Matrix $\hat{X}$ die Spaltenmatrix der Vektoren $\hat{\vect{x}}^c$ ist.
Mit $\tilde{Y} = D^\frac{1}{2} \hat{X}$ lautet das Spurmaximierungsproblem für den Normalized Cut folgendermaßen:
\begin{align}
\label{formula:ncut-tracemax}
	\max_{\tilde{Y}}{} \tr{ \tilde{Y}^T D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \tilde{Y} }
\end{align}
Das Problem in~\ref{formula:ncut-tracemax} ist äquivalent zu~\ref{formula:wkkm-tracemax}, wenn wir als
Gewichtsmatrix $W = D$ und als Kernel-Matrix $K = D^{-1} A D^{-1}$ setzen.
\absatz
Wir wollen abschließend noch anmerken, dass die Adjazenzmatrix $A$ positiv definit sein muss, damit sie als Kernel-Matrix in
Algorithmus~\ref{algo:wkkm} garantiert zu einer iterativen Verringerung des gewichteten Kernel-$k$-means Zielfunktionswertes
führt. Kernel-Matrizen müssen zudem generell positiv definit sein, damit sie Skalarprodukte in einem anderen Vektorraum
berechnen~\cite{Shawe-TaylorC04}.
Um dies in der Praxis sicherzustellen, ist bei der Wahl der Kernel-Matrix gegebenenfalls das Aufaddieren einer
"`Verschiebung"' auf die ursprüngliche Kernel-Matrix nötig. Wir gehen auf dieses Detail im Experimente-Kapitel noch genauer ein.

\subsection{Die Kernel Methode für \texorpdfstring{$k$}{k}-means\texttt{++}}
\label{subsection:kernelkmpp}

Ein wichtiges Merkmal von Algorithmus~\ref{algo:wkkm} ist, dass er grundsätzlich ohne Methoden der spektralen Clusteranalyse
auskommt. Wir müssen uns jedoch wie auch bei Lloyds Algorithmus ohne Kernel Methode mit der geeigneten initialen Wahl
der Clusterzentren beschäftigen. Zunächst einmal wählen wir diese wie schon zuvor zufällig aus der Eingabepunktmenge.
Diese Vorgehensweise ist durchaus praktikabel, die experimentelle Evaluation aus~\citep{DhillonGK04,DhillonGK07} zeigt jedoch,
dass sich mit einer geschickteren Wahl die Qualität des letztlich berechneten Clusterings noch signifikant steigern lässt.
Die Autoren verwenden für die Initialisierung die spektrale Clusteranalyse, was wiederum den Aufwand der Eigenvektor-Berechnung
mit sich bringt.
\absatz
Konkret wird mit einem Verfahren aus~\cite{GolubL96} eine Problemrelaxion
von~\ref{formula:wkkm-tracemax}/~\ref{formula:ratioassoc-tracemax} beziehungsweise
von~\ref{formula:wkkm-tracemax}/~\ref{formula:ncut-tracemax} gelöst.
Relaxieren wir die Wahl von $Y$ beziehungsweise $\tilde{Y}$ auf eine beliebige
orthonormale Matrix, dann sind diese von der Form $Y = V_k Q$ beziehungsweise $\tilde{Y} = \tilde{V}_k Q$.
Dabei ist $Q$ eine beliebige orthogonale $k \times k$-Matrix, $V_k$ die Matrix der $k$ größten Eigenvektoren von
$W^{\frac{1}{2}} K W^{\frac{1}{2}}$ und $\tilde{V}_k$ die Matrix der $k$ größten Eigenvektoren von
$D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$.

Die Berechnung der jeweiligen $k$ größten Eigenvektoren wird mit wachsender Clusteranzahl
$k$ aufwändig, wie wir schon in Abschnitt~\ref{subsection:basics:kernel-spectral} beobachtet haben.
Intuitiv ist daher die Fragestellung, ob es möglich ist, eine Initialisierung ohne Eigenvektor-Berechnungen vorzunehmen,
die dennoch deutlich verbesserte Clusterings mit dem Kernel-$k$-means-Algorithmus ermöglicht.

In Abschnitt~\ref{subsection:basics:clustering} haben wir bereits den Algorithmus \kmpp{} von Arthur und
Vassilvitskii~\cite{ArthurV07} kennengelernt. Die initiale Wahl der Cluster erfolgt hier gemäß einer
Wahrscheinlichkeitsverteilung der Punkte als mögliche Clusterzentren, die proportional zur quadrierten euklidischen Distanz
zum jeweils nächstgelegenen bereits gewählten Zentrum ist. Wir zeigen nun, dass die Kernel Methode auch auf den Algorithmus
\kmpp{} angewendet werden kann. Damit ist es uns möglich, Algorithmus~\ref{algo:kernel-lloyd} (und damit
auch Algorithmus~\ref{algo:wkkm}) in eine Kernel-basierte Variante von \kmpp{} umzuwandeln, die wir \kkmpp{} nennen.
\absatz
Wir erinnern uns, dass wir im Zusammenhang mit \kmpp{} für einen Punkt $x$ aus der Eingabe-Punktmenge $P$ mit $D(x)$
die geringste Distanz von $x$ zum nächstgelegenen bereits gewählten Zentrum notieren. Der Punkt $x$ wird als nächstes
Zentrum mit der Wahrscheinlichkeit
\[ \frac{D(x)^2}{\sum_{y \in P} D(y)^2} \]
gewählt. Wir können demnach die Kernel Methode auf den Algorithmus \kmpp{} anwenden, wenn wir für jeden Eingabepunkt $x$ den
Wert $D(\phi(x))$ beziehungsweise $D(\phi(x))^2$ berechnen können. Dazu zeigen wir nun, dass wir diese Werte wie schon zuvor
beim Kernel-$k$-means-Algorithmus ausschließlich über die Kernelfunktion $\kappa$ ohne explizite Kenntnis der abgebildeten
Punkte oder der Abbildung $\phi$ selbst berechnen können.

Wir nehmen zunächst an, dass die Zentren $c_1, \dots, c_l$ mit $l \in \{ 1, \dots, k-1 \}$ bereits gewählt worden sind.
Dann können wir $D(x)$ formal notieren als das Minimum über alle quadrierten euklidischen Distanzen von $x$ zu jedem der bisher
gewählten Zentren $c_i$:
\[ D(x) = \min_{i = 1 \dots l} \EuclidSquared{x}{c_i} \]
Wenden wir die Kernel Methode an, berechnet sich $D(\phi(x))$ dementsprechend folgendermaßen:
\[ D(\phi(x)) = \min_{i = 1 \dots l} \EuclidSquared{\phi(x)}{\phi(c_i)} \]
Demnach genügt es, zu zeigen, dass wir $\EuclidSquared{\phi(x)}{\phi(c_i)}$ ausschließlich mit der Kernelfunktion berechnen
können. Wir haben bereits gesehen, dass die Kernelfunktion gerade dafür gedacht ist. Es gilt:
\begin{align}
	\EuclidSquared{\phi(x)}{\phi(c_i)} = \kappa(x,x) - 2 \kappa(x,c_i) + \kappa(c_i,c_i)
\end{align}
Mit denselben Überlegungen können wir auch zeigen, dass sich der Wert von $D(\phi(x))^2$ mit der Kernel Methode berechnen lässt.
Zunächst lässt sich auch $D(\phi(x))^2$ als Minimum formulieren:
\[ D(\phi(x))^2 = \left( \min_{i = 1 \dots l} \EuclidSquared{\phi(x)}{\phi(c_i)} \right)^2 \]
Dazu müssen wir effektiv $\left( \EuclidSquared{\phi(x)}{\phi(c_i)} \right)^2$ berechnen. Auch dies lässt sich mit der
Kernelfunktion umsetzen:
\begin{align}
\label{formula:kkmpp}
	\left( \EuclidSquared{\phi(x)}{\phi(c_i)} \right)^2 &= \left( \kappa(x,x) - 2 \kappa(x,c_i) + \kappa(c_i,c_i) \right)^2 \nonumber \\
	&= \left(\kappa(x,x)\right)^2 - 4 \kappa(x,x) \kappa(x,c_i) + 2 \kappa(x,x) \kappa(c_i,c_i) \nonumber \\
	& \; \; \; + 4 \kappa(x,c_i) \kappa(x,c_i) - 4 \kappa(x,c_i) \kappa(c_i,c_i) + \left(\kappa(c_i,c_i)\right)^2
\end{align}
Der Algorithmus \kkmpp{} wird damit die logische Weiterentwicklung von \kmpp{} auf Basis der Kernel Methode.

\begin{algorithm}
\label{algo:kernelkmeanspp}
\caption{\kkmpp}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$, Kernelfunktion $\kappa$}
	\Output{$k$ initiale Clusterzentren für $P$}
	\BlankLine
	
	% Algo
	Wähle $c_1$ zufällig gleichverteilt aus $P$\;
	\For{$i \leftarrow 2$ \KwTo $k$}{
		Berechne $D(\phi(x'))^2$ mit~\ref{formula:kkmpp} für alle $x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$\;
		Wähle den Punkt $x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$ als Zentrum $c_i$ mit Wahrscheinlichkeit $\frac{D(\phi(x'))^2}{\sum_{x \in P} D(\phi(x))^2}$\;
	}
	Führe den Kernel-$k$-means-Algorithmus mit den initialen Clusterzentren $c_1, \dots, c_k$ aus.
\end{algorithm}
Wir zeigen nun, dass auch \kkmpp{} eine $\BigO{\log k}$-Approximation für das $k$-means-Problem ist. Strukturell gehen
wir dabei analog zu dem Papier von Arthur und Vassilvitskii~\cite{ArthurV07} vor, da sich durch die Kernel Methode keine
wesentlichen Änderungen ergeben. Für ein Clustering $C = \{ C_1, \dots, C_k \}$ mit den jeweiligen Clusterzentren
$c_1, \dots, c_k$ bezeichnen wir mit dem \emph{Potenzial} $\theta$ den Zielfunktionswert des Clusterings bezüglich der
Kernel-$k$-means-Zielfunktion, das heißt
\[ \theta(C) = \sum_{i=1}^k \theta(C_i) = \sum_{i=1}^k \sum_{p \in C_i} \EuclidSquared{\phi(p)}{\phi(c_i)} \]
Das Potenzial des optimalen Clusterings $C_\textrm{OPT}$ notieren wir mit $\optpot$. Bevor wir mit der Analyse beginnen,
benötigen wir folgendes Lemma, das einen beliebigen Punkt mit einer Punktmenge und ihrem Zentroiden in Relation setzt:

\begin{lemma}
\label{lemma:point-centroid}
	Sei $V$ ein euklidischer Vektorraum und $S \subset V$ eine Vektormenge mit dem geometrischen
	Zentroiden $z(S) = \frac{1}{\left|S\right|} \sum_{\vect{x} \in P} \vect{x}$.
	Weiterhin sei $\vect{q} \in V$ ein beliebiger Vektor aus $V$. Es gilt:
	\[ \sum_{\vect{x} \in S} \EuclidSquared{\vect{x}}{\vect{q}} - \sum_{\vect{x} \in S} \EuclidSquared{\vect{x}}{z(S)}
		= \left|S\right| \cdot \EuclidSquared{z(S)}{\vect{q}} \]
\end{lemma}
Wir zeigen nun das eigentliche Ergebnis.

\begin{satz}[\kkmpp{} ist $\BigO{\log k}$-kompetitiv]
\label{thm:kkmpp-ologk-approx}
	Wenn das Clustering $C$ mit \kkmpp{} berechnet wird, dann gilt für das Potenzial $\theta$ von $C$:
	\[ E[\theta(C)] \leq 8 (\ln{k} + 2) \optpot \]
\end{satz}
Der Beweis erfolgt in drei Schritten. Wir zeigen zunächst, dass das erste zufällig gleichverteilt gewählte Clusterzentrum
die Approximationseigenschaft erfüllt. Dieselbe Eigenschaft zeigen wir im zweiten Schritt für die übrigen Zentren, die gemäß
der $D^2$-Gewichtung gewählt werden. Im dritten und letzten Schritt beweisen wir mittels Induktion das Gesamtergebnis.

\begin{lemma}[Das erste Zentrum ist kompetitiv]
\label{lemma:kkmpp-first-center-approx}
	Sei $A$ ein beliebiges Cluster von $C_\textrm{OPT}$ und sei $C$ ein Clustering mit genau einem Zentrum, das zufällig
	gleichverteilt aus $A$ gewählt wird. Dann gilt:
	\[ E[\theta(A)] = 2 \optpot(A) \]
\end{lemma}
\begin{proof}
	Sei $z(A)$ der geometrische Zentroid der Punkte in $A$, das heißt
	\[ \phi(z(A)) = \frac{1}{\left|A\right|} \sum_{\vect{p} \in A} \phi(\vect{p}) \]
	Mit Lemma~\ref{lemma:point-centroid} gilt:
	\begin{align*}
		& \sum_{\vect{p} \in A} \frac{1}{\left|A\right|} \cdot \left( \sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p})}{\phi(\vect{p}')} \right) \\
		&= \frac{1}{\left|A\right|} \sum_{\vect{p} \in A} \left( \sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(z(A))} 
			+ \left|A\right| \cdot \EuclidSquared{\phi(\vect{p})}{\phi(z(A))} \right) \\
		&= 2 \sum_{\vect{p} \in A} \EuclidSquared{\phi(\vect{p})}{\phi(z(A))} \\
		&= E[\theta(A)]
	\end{align*}
\end{proof}
Wir zeigen nun, dass die Approximationseigenschaft auch für die übrigen Zentren, die gemäß der $D^2$-Gewichtung gewählt werden,
gilt.
\begin{lemma}[Die übrigen Zentren sind kompetitiv]
\label{lemma:kkmpp-remaining-center-approx}
	Sei $A$ ein beliebiges Cluster von $C_\textrm{OPT}$ und sei $C$ ein beliebiges Clustering. Wenn ein Zentrum aus $A$
	zufällig nach der $D^2$-Gewichtung zu $C$ hinzugefügt wird, dann gilt:
	\[ E[\theta(A)] \leq 8 \optpot(A) \]
\end{lemma}
\begin{proof}
	Ein Punkt $\vect{p} \in A$ wird mit der Wahrscheinlichkeit
	\[ \frac{D(\phi(\vect{p}))^2}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2} \]
	als Zentrum gewählt. Wenn der Punkt $\vect{p}$ als Zentrum gewählt wird, beträgt der Potenzial-Wert eines Punktes $\vect{p'}$,
	der dem Zentrum $\vect{p}$ zugeordnet ist, genau
	\[ \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \]
	Der Erwartungswert für das Potenzial des gesamten Clusters $A$ ist dementsprechend:
	\[  E[\theta(A)] = \sum_{\vect{p} \in A} \frac{D(\phi(\vect{p}))^2}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2}
		\sum_{\vect{p}' \in A} \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \]
	Für die Berechnung von $D(\phi(\vect{p}'))$ und der Distanzen $\Euclid{\phi(\vect{p}')}{\phi(\vect{p})}$ verwenden
	wir im \kkmpp-Algorithmus die Kernelfunktion $\kappa$. Wenn die Kernelfunktion, beziehungsweise die zugehörige
	Kernelmatrix \emph{positiv definit} ist (für Details siehe beispielsweise~\cite{Shawe-TaylorC04}), gilt für diese die
	Cauchy-Schwarz-Ungleichung, also $\left|\kappa(x,y)\right|^2 \leq \kappa(x,x) \cdot \kappa(y,y)$.
	Aus der Cauchy-Schwarz-Ungleichung folgen die Dreiecks-Ungleichung, sowie die Power-Mean-Ungleichung bezüglich der
	Kernelfunktion. Die üblichen Kernelfunktionen wie beispielsweise in Tabelle~\ref{tbl:kernel-functions} sowie die
	in unseren Experimenten eingesetzten Kernel haben eine positiv definite zugehörige Kernelmatrix. Wir können daher an
	dieser Stelle die Dreiecks-Ungleichung anwenden und erhalten für alle $\vect{p}, \vect{p}'$:
	\[ D(\phi(\vect{p})) \leq D(\phi(\vect{p}')) + \Euclid{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Für die $D^2$-Gewichtung gilt zudem mit der Power-Mean-Ungleichung:
	\[ D(\phi(\vect{p}))^2 \leq 2 D(\phi(\vect{p}'))^2 + 2 \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Summiert über alle Punkte $\vect{p} \in A$ erhalten wir somit:
	\[ D(\phi(\vect{p}))^2 \leq \frac{2}{\left|A\right|} \sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2
		+ \frac{2}{\left|A\right|} \sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Wir schätzen nun $E[\theta(A)]$ nach oben ab:
	\begin{align*}
		E[\theta(A)] 	&\leq \frac{2}{\left|A\right|} \cdot \sum_{\vect{p} \in A} \frac{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2}
								\cdot \sum_{\vect{p}' \in A} \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \\
						& \; \; \; \; \; + \frac{2}{\left|A\right|} \cdot
							\sum_{\vect{p} \in A} \frac{\sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})}}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2} \\
						& \; \; \; \; \; \; \cdot \sum_{\vect{p}' \in A} \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \}
	\end{align*}
	Im ersten Summanden schätzen wir
	\[ \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \leq \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	nach oben ab, und im zweiten Summanden
	\[ \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \leq D(\phi(\vect{p}')) \]
	Damit erhalten wir vereinfacht:
	\[ E[\theta(A)] \leq \frac{4}{\left|A\right|} \sum_{\vect{p} \in A} \sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Nun können wir Lemma~\ref{lemma:kkmpp-first-center-approx} anwenden und erhalten:
	\[ E[\theta(A)] \leq 8 \optpot(A) \]
\end{proof}
Aus den Lemmas~\ref{lemma:kkmpp-first-center-approx} und~\ref{lemma:kkmpp-remaining-center-approx} folgt, dass
die Wahl der Zentren von \kkmpp{} der Approximationseigenschaft genügt, solange der Algorithmus Zentren aus jedem Cluster
des optimalen Clusterings $C_\textrm{OPT}$ auswählt. Im dritten und letzten Schritt unserer Analyse zeigen wir nun,
dass der entstehende Fehler im anderen Fall nach oben insgesamt durch $\BigO{\log k}$ beschränkt ist.

\begin{lemma}
\label{lemma:kkmpp-error}
	Sei $C$ ein beliebiges Clustering. Gegeben seien $u > 0$ viele freie Cluster (die übrigen Cluster nennen wir abgedeckt)
	von $C_\textrm{OPT}$. $P_u$ seien die Punkte in
	diesen Clustern. Weiterhin sei $P_c = P - P_u$. Wenn $t \leq u$ zufällig gemäß der $D^2$-Gewichtung gewählte Zentren zu
	$C$ hinzugefügt werden, dann gilt für das erwartete neue Potenzial $E[\theta'(C)]$ von $C$:
	\[ E[\theta'(C)] \leq \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_t) + \frac{u-t}{u} \cdot \theta(P_u) \]
	Dabei ist $H_t$ die $t$-te harmonische Zahl $H_t = 1 + \frac{1}{2} + \dots + \frac{1}{t}$.
\end{lemma}

\begin{proof}
	Wir beweisen die Aussage durch vollständige Induktion für $(t-1, u)$ und $(t-1, u-1)$.
	\paragraph{Induktionsanfang.} Falls $t=0$ und $u > 0$, gilt:
	\[ 1 + H_t = \frac{u-t}{u} = 1 \]
	Wir betrachten nun den Fall, dass $t = u = 1$. Wir wählen genau ein neues Zentrum aus der Menge von genau einem freien Cluster
	mit der Wahrscheinlichkeit $\frac{\theta(P_u)}{\theta(C)}$. In diesem Fall folgt aus
	Lemma~\ref{lemma:kkmpp-remaining-center-approx}, dass
	\[ E[\theta'(C)] \leq \theta(P_c) + 8 \optpot(P_u) \]
	Auch wenn wir ein Zentrum aus einem abgedeckten Cluster wählen, gilt $\theta'(C) \leq \theta(C)$.
	Somit können wir das erwartete neue Potenzial in diesem Fall nach oben abschätzen:
	\begin{align*}
		E[\theta'(C)] &\leq \frac{\theta(P_u)}{\theta(C)} \cdot \left( \theta(P_c) + 8 \optpot(P_u) \right)
							+ \frac{\theta(P_u)}{\theta(C)} \cdot \theta(C) \\
					  &\leq 2 \theta(P_c) + 8 \optpot(P_u)
	\end{align*}
	Bezüglich der Behauptung gilt in diesem Fall, dass $1 + H_t = 1 + 1 = 2$ (und $\frac{u-t}{u} = \frac{1-1}{1} = 0$).
	Daher gilt die Behauptung auch in diesem Fall.
	\paragraph{Induktionsschritt.} Wir nehmen eine Fallunterscheidung bezüglich des Clusters vor, aus dem wir unser erstes
	Zentrum wählen.
	\begin{enumerate}
		\item[(a)] 	Wir nehmen an, das erste gewählte Zentrum stammt aus einem abgedeckten Cluster. Analog zum Induktionsanfang
					ist die Wahrscheinlichkeit dafür $\frac{\theta(P_c)}{\theta(C)}$. Da das Zentrum aus einem abgedeckten
					Cluster stammt, kann es $\theta(C)$ nicht vergrößern. Wir verwenden die Induktionsannahme mit $t-1$.
					Folglich tragen wir erwartet höchstens den folgenden Betrag zu $E[\theta'(C)]$ bei:
					\[ \frac{\theta(P_c)}{\theta(C)} \cdot \left( \left( \theta(P_c) + 8 \optpot(P_u) \right)
						\cdot (1 + H_{t-1}) + \frac{u - (t-1)}{u} \cdot \theta(P_u) \right) \]
		\item[(b)] 	Wir nehmen an, das erste gewählte Zentrum stammt aus einem freien Cluster $A$. Auch hier ist die
					Wahrscheinlichkeit für dieses Ereignis $\frac{\theta(A)}{\theta(C)}$. Für alle $a \in A$ sei
					$p_a$ die Wahrscheinlichkeit, dass wir $a$ als Zentrum wählen. Weiterhin sei $\theta_a$ das Potenzial
					$\theta(A)$, nachdem $a$ als Zentrum gewählt wurde. Wir wenden wieder die Induktionsannahme an, setzen
					jedoch $t = t-1$, $u = u-1$ und fügen $A$ zur Menge der abgedeckten Cluster hinzu. Dann tragen wir
					erwartet höchstens den folgenden Betrag zu $E[\theta'(C)]$ bei:
					\begin{align*}
						& \frac{\theta(A)}{\theta(C)} \cdot \sum_{a \in A} p_a \left(
							\left( \theta(P_c) + \theta_a + 8 \optpot(P_u) - 8 \optpot(A) \right)
							\right) \\
						& \; \; \; \cdot (1 + H_{t-1}) + \frac{u - (t-1)}{u-1} \cdot \left(
							\theta(P_u) - \theta(A) \right) \\
						&\leq \frac{\theta(A)}{\theta(C)} \cdot \bigg( \left( \theta(P_c) + 8 \optpot(P_u) \right)
							\cdot (1 + H_{t-1}) \\
						& \; \; \; + \frac{u-t}{u-1} \cdot \left( \theta(P_u) - \theta(A) \right) \bigg)
					\end{align*}
					Die zweite Abschätzung gilt, da $\sum_{a \in A} p_a \leq 8 \optpot(A)$ direkt aus
					Lemma~\ref{lemma:kkmpp-remaining-center-approx} folgt. An dieser Stelle müssten wir über alle freien
					Cluster summieren, um den gesamten Potenzial-Beitrag nach oben abzuschätzen. Dazu setzen wir die
					Power-Mean-Ungleichung ein. Mit dieser gilt:
					$ \sum_{A \subset P_u} \theta(A^2) \geq \frac{1}{u} \cdot \theta(P_u)^2 $.
					Damit ist es uns nun möglich, den über alle freien Cluster aufsummierten Beitrag zum Potenzial
					nach oben zu beschränken:
					\begin{align*}
						&\frac{\theta(P_u)}{\theta(C)} \cdot \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_{t-1}) \\
						& \; \; \; + \frac{1}{\theta(C)} \cdot \frac{u-t}{u-1} \cdot \left( 
						\theta(P_u)^2 - \frac{1}{u} \cdot \theta(P_u)^2 \right)
					\end{align*}
					Dieser Term lässt sich etwas vereinfachen zu:
					\[ \frac{\theta(P_u)}{\theta(C)} \cdot \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_{t-1})
						+ \frac{u-t}{u} \cdot \theta(P_u) \]
	\end{enumerate}
	Wir kombinieren nun den erwarteten Potenzial-Beitrag zu $\theta'(C)$ aus den Fällen (a) und (b) und erhalten die folgende
	obere Schranke:
	\begin{align*}
		E[\theta'(C)] &\leq \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_{t-1})
			+ \frac{u-t}{u} \cdot \theta(P_u) + \frac{\theta(P_c)}{\theta(C)} \cdot \frac{\theta(P_u)}{u} \\
			&\leq \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot \left( 1 + H_{t-1} + \frac{1}{u} \right)
				+ \frac{u-t}{u} \cdot \theta(P_u)
	\end{align*}
	In der Aussage von Lemma~\ref{lemma:kkmpp-error} ist $t \leq u$ vorausgesetzt. Daher gilt $\frac{1}{u} \leq \frac{1}{t}$.
	Damit folgt der Induktionsschluss.
\end{proof}
Wir können Satz~\ref{thm:kkmpp-ologk-approx} als Spezialfall von Lemma~\ref{lemma:kkmpp-error} beweisen. Dazu betrachten wir
das Clustering $C$, welches wir erhalten, nachdem die Zentren-Wahl von \kkmpp{} abgeschlossen ist und bevor der Kernel-$k$-means
Algorithmus ausgeführt wird. Sei $A$ das Cluster aus $C_\textrm{OPT}$, aus dem wir das erste Zentrum ausgewählt haben.
Wir wenden Lemma~\ref{lemma:kkmpp-error} mit $t = u = k-1$ an und legen dazu $A$ als einziges abgedecktes Cluster fest.
Dann gilt:
\[ E[\optpot] \leq \left( \theta(A) + 8 \optpot - 8 \optpot(A) \right) \cdot (1 + H_{k-1}) \]
Es ist allgemein bekannt, dass für $t \geq 2$ die $t$-te harmonische Zahl nach oben beschränkt ist durch $H_t < \ln{t} + 1$.
Damit gilt insbesondere $H_{k-1} \leq \ln{k} + 1$. Somit gilt:
\[ E[\optpot] \leq \left( \theta(A) + 8 \optpot - 8 \optpot(A) \right) \cdot (\ln{k} + 2) \]
Satz~\ref{thm:kkmpp-ologk-approx} folgt mit Lemma~\ref{lemma:kkmpp-first-center-approx}.

\subsection{Eine praktische Kernmengenkonstruktion}

In diesem Abschnitt wollen wir die Kernmengenkonstruktion von Feldman, Schmidt und Sohler~\cite{FeldmanSS13,Schmidt14} auf ihre
Implementierbarkeit untersuchen und gegebenenfalls für eine praktische Umsetzung modifizieren.

Bevor wir die Konstruktion erläutern, benötigen wir zwei Konzepte, die zum Verständnis nötig sind. Zunächst wird mit der
Konstruktion streng genommen eine Kernmenge \emph{mit Verschiebung} $\Delta$ berechnet. Diese unterscheidet sich von einer
starken Kernmenge lediglich in der zusätzlichen Addition der Konstante $\Delta$ in den Kosten der Kernmenge.
Formal ist eine Kernmenge mit Verschiebung folgendermaßen definiert.

\begin{definition}[Kernmenge mit Verschiebung~\citep{FeldmanSS13}]
\label{def:coreset-with-offset}
	Sei $P \subset \Rd$ eine endliche Punktmenge und sei $w_1 : P \rightarrow \mathbb{R}$ eine Gewichtsfunktion, die jedem
	Punkt in $P$ ein Gewicht zuordnet. Sei weiterhin $\epsilon \in (0,1)$ und $\Delta \in \mathbb{R}$.
	Eine endliche Menge $P \subset \Rd$ und eine
	Gewichtsfunktion $w_2 : S \rightarrow \mathbb{R}$ bilden eine Kernmenge mit Verschiebung $\Delta$ für $P$, wenn
	für alle Mengen von $k$ Zentren $C \subset P$ gilt:
	\[ (1-\epsilon) \sum_{x \in P} w_1(x) \min_{c \in C} \EuclidSquared{x}{c} 
		\leq \sum_{y \in S} w_2(y) \min_{c \in C} \EuclidSquared{y}{c} + \Delta
		\leq  (1+\epsilon) \sum_{x \in P} w_1(x) \min_{c \in C} \EuclidSquared{x}{c} \]
\end{definition}

Weiterhin liegt der Kernmengenkonstruktion die bereits erwähnt Erkenntnis zu Grunde, dass die optimale Lösung eines
$1$-means-Problems für eine Punktmenge $P$ der geometrische Zentroid $z(P)$ ist. Mit dem folgenden Lemma lassen sich die
$1$-means-Kosten einer Punktmenge $P$ mit einem gegebenen Zentrum $c$ aufteilen in die optimalen $1$-means-Kosten von $P$
plus das $\left|P\right|$-fache der Distanz von $c$ zum optimalen Zentrum, also dem Zentroiden.

\begin{lemma}
\label{lemma:1-means-cost-parts}
	Sei $P \subset \Rd$ eine endliche Punktmenge mit dem geometrischen Zentroiden $z(P)$. Für jeden Punkt $q \in \Rd$ gilt:
	\[ \sum_{x \in P} \EuclidSquared{x}{q} = \sum_{x \in P} \EuclidSquared{x}{z(P)} + \left|P\right| \cdot \EuclidSquared{q}{z(P)}
	 \]
	 Wenn $P$ eine gewichtete Punktmenge mit Gewichtsfunktion $w : P \rightarrow \mathbb{R}^+$ ist, dann gilt:
	 \[ \sum_{x \in P} w(x) \cdot \EuclidSquared{x}{q} = \sum_{x \in P} w(x) \cdot \EuclidSquared{x}{z_w(P)} +
	 	W \cdot \EuclidSquared{q}{z_w(P)} \]
	 Dabei ist $W = \sum_{x \in P} w(x)$ und $z_w(P) = \frac{1}{W} \sum_{x \in P} w(x) \cdot x$.
\end{lemma}

Die Kernmengenkonstruktion versucht auszunutzen, dass der Anteil der optimalen $1$-means-Kosten konstant in allen möglichen
Clusterings ist. Die Berechnung des zweiten Summanden gestaltet sich als schwierig. Die Konstruktion ist der Versuch, diese
Erkenntnisse auf das $k$-means-Problem zu verallgemeinern. Gegeben ein Clustering $C$, lässt sich die Punktmenge $P$ in
$k$ Teile partitionieren, sodass der Zentroid und die $1$-means-Kosten jeder Partition ausreichen, um die Gesamtkosten von $C$
zu berechnen. $C$ ist jedoch zuvor nicht bekannt.

Der Algorithmus partitioniert die ursprüngliche Punktmenge $P$ rekursiv in Teilmengen, bis es für eine Teilmenge $P'$ ausreicht,
eine $1$-means-Kernmenge zu verwenden, um die $k$-means-Kosten von $P'$ zu approximieren. Sobald dies für alle Teilmengen
möglich ist, wird nicht länger partitioniert und die Teilmengen werden durch ihre Zentroiden ersetzt. Die Zentroiden bilden
dann die Punkte der Kernmenge, deren Gewicht die Kardinalität der jeweiligen Teilmenge ist. Wir unterscheiden bei den
Teilmengen zwischen \emph{$k$-unstrukturierten} Teilmengen und \emph{$k$-strukturierten} Teilmengen. Eine $k$-unstrukturierte
Teilmenge muss nicht weiter partitioniert werden. Für sie gilt, dass ihre $1$-means-Kosten höchstens um einen Faktor
$(1+\epsilon')$ größer sind als die $k$-means-Kosten. Die $k$-strukturierten Teilmengen werden jeweils solange weiter
partitioniert, bis nur noch $k$-unstrukturierte Teilmengen übrig sind. In der ursprünglichen Version des Algorithmus werden
die $k$-strukturierten Teilmengen anhand der optimalen $k$-means-Lösung partitioniert. Alternativ wird die rekursive
Partitionierung abgebrochen, wenn eine maximale Rekursionstiefe $\nu$ erreicht ist, um sicherzustellen, dass nicht zu lange
partitioniert wird.

\begin{algorithm}
\caption{\texttt{Coreset}~\cite{FeldmanSS13,Schmidt14}}
\label{algo:fss-coreset}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$, $\epsilon$}
	\Output{Kernmenge $S$ mit Gewichten $w$ und Verschiebung $\Delta$}
	\BlankLine
	
	% Algo
	$\epsilon' \leftarrow \epsilon^2/50$\;
	$S \leftarrow \emptyset$\;
	$\Delta \leftarrow 0$\;
	$M \leftarrow $ \texttt{Partition}($P,k,0,\lceil \log_{1+\epsilon'} 1/\epsilon' \rceil, \epsilon'$)\;
	\For{jede Teilmenge $P' \in M$}{
		$S \leftarrow S \cup z(P')$\;
		$w(z(P')) \leftarrow \left|P'\right|$\;
		$\Delta \leftarrow \Delta + \sum_{x \in P'} \EuclidSquared{x}{z(P')}$\;
	}
	\Return{$S, w, \Delta$}
\end{algorithm}
Algorithmus~\ref{algo:fss-coreset} ist eine Pseudocode-Beschreibung der Kernmengenkonstruktion. Die Punkte der Kernmenge
werden in $S$ gespeichert, die Gewichtung in $w$ und die Verschiebung in $\Delta$. Wie zuvor beschrieben ruft \texttt{Coreset}
die rekursive Partitionierungs-Routine \texttt{Partition} auf und ersetzt am Ende der Partitionierung die Teilmengen durch
ihren Zentroiden, der dann als Punkt in die Kernmenge aufgenommen wird.
\absatz
Die Partitionierung nimmt der Algorithmus~\ref{algo:fss-partition} \texttt{Partition} vor. In der Menge $M$ speichert diese
alle berechneten Teilmengen. Die Zeilen 1-3 berechnen dabei eine neue Partitionierung, was in der ursprünglichen Variante
des Algorithmus anhand einer optimalen $k$-means-Lösung erfolgt. Die Unterscheidung nach $k$-strukturierten und
$k$-unstrukturierten Teilmengen erfolgt in Zeile 4. Der rekursive Aufruf der Partitionierung erfolgt in den Zeilen 7-8.

\begin{algorithm}
\caption{\texttt{Partition}~\cite{FeldmanSS13,Schmidt14}}
\label{algo:fss-partition}
	% Optionen
	\DontPrintSemicolon
	\SetKw{KwOr}{\textbf{or}}
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$, $t, \nu, \epsilon'$}
	\Output{Partitionierung $M$}
	\BlankLine
	
	% Algo
	Berechne optimale Menge von Zentren $C^* = \{c_1, \dots, c_k\}$ für $P$\;
	Sei $P_1, \dots, P_k$ die durch $C^*$ induzierte Partitionierung von $P$\;
	Sei $z(P)$ der Zentroid von $P$\;
	\If{$t=\nu$ \KwOr $\sum_{x \in P} \EuclidSquared{x}{z(P)} \leq (1+\epsilon') \sum_{i=1}^{k} \sum_{y \in P_i} \EuclidSquared{y}{z(P_i)} $}{
		$M \leftarrow M \cup P$\;	
	}\Else{
		\For{$i \leftarrow 1$ \KwTo $k$}{
			$M \leftarrow M \cup $ \texttt{Partition}($P_i,k,t+1,\nu,\epsilon'$)\;
		}
	}
	\Return{$M$}
\end{algorithm}
In~\cite{Schmidt14} wird bereits thematisiert, dass die wiederholte Partitionierung anhand einer optimalen $k$-means-Lösung
für die Praxis zu aufwändig wäre und zu unpraktikablen Laufzeiten führen würde. Schmidt schlägt daher vor, eine
$\alpha$-approximative Lösung zu verwenden. Dementsprechend wird dann in Zeile 1 von Algorithmus~\ref{algo:fss-partition}
eine Lösung $C'$ berechnet, deren Kosten höchstens um einen Faktor $\alpha$ größer sind, als die Kosten einer optimalen
$k$-means-Lösung für $P$. Um zu entscheiden, ob die Teilmengen $k$-strukturiert sind, wird dann geprüft, ob
\[ \sum_{x \in P'} \EuclidSquared{x}{z(P')} \leq ((1+\epsilon')/\alpha) \sum_{i=1}^{k} \sum_{y \in P_i} \EuclidSquared{y}{z(P_i)} \]
gilt. Die Analyse in~\cite{Schmidt14} zeigt, dass auch bei Verwendung der Approximation noch Garantien über Qualität und
Laufzeit bewiesen werden können. Insbesondere berechnet der entsprechend modifizierte Algorithmus weiterhin eine
$(1+\epsilon)$-Kernmenge mit Verschiebung $\Delta$.
\absatz
Wir wollen noch einen Schritt weiter gehen und die Partitionierung heuristisch vornehmen. Die Schwierigkeit besteht weiterhin
darin, eine heuristische Lösung für das $k$-means-Problem im Partitionierungsschritt zu berechnen, die nur geringen Aufwand
hat und dennoch in vielen Fällen Partitionierungen mit handhabbaren Kosten erzeugt. Wie schon zuvor sehen wir von einer
vollkommen zufälligen Auswahl der Zentren ab und setzen stattdessen die $D^2$-Gewichtung von \kmpp{} ein. Dabei gehen
wir folgendermaßen vor. Zunächst wählen wir initial $c \cdot k$ Zentren gemäß der $D^2$-Gewichtung aus der gesamten
Punktmenge. Diese induzieren eine initiale Partitionierung $P_1, \dots, P_{ck}$, die wir bestimmen, indem wir jeden
Punkt dem nächstgelegenen Zentrum zuweisen. Anschließend führen wir rekursiv die folgende modifizierte, weitere Partitionierung
durch.
Bis zu einer maximalen Tiefe $T$ (entspricht $\nu$ aus der ursprünglichen Konstruktion) prüfen wir für jede
Teilmenge $P'$, ob eine Partitionierung in weitere $d \cdot k$ Teilmengen um einen Faktor $f$ billiger ist, als die
$1$-means-Kosten mit dem Zentroiden $z(P')$. Die $d \cdot k$ Teilmengen in der rekursiven Partitionierung wählen wir dabei
wieder gemäß der $D^2$-Gewichtung in der jeweiligen Teilmenge aus. Wenn eine weitere Partitionierung anhand der $d \cdot k$
Zentren vorzuziehen ist, berechnen wir die neuen Teilmengen wieder über Zuweisung der Punkte zu den nächstgelegenen Zentren.

\begin{algorithm}
\caption{\texttt{Coreset2}}
\label{algo:coreset}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$, $c, d, f, T$}
	\Output{Kernmenge $S$ mit Gewichten $w$ und Verschiebung $\Delta$}
	\BlankLine
	
	% Algo
	$l \leftarrow c \cdot k$\;
	$S \leftarrow \emptyset$\;
	$M \leftarrow \emptyset$\;
	$\Delta \leftarrow 0$\;
	Wähle $l$ Zentren $C^0 = \{c_1, \dots, c_l\}$ gemäß $D^2$-Gewichtung aus $P$\;
	Sei $P_1, \dots, P_l$ die durch $C^0$ induzierte Partitionierung von $P$\;
	\For{$i \leftarrow 1$ \KwTo $l$}{
		$M_i \leftarrow $ \texttt{Partition2}($P_i,k,d,f,1,T$)\;
		$M \leftarrow M \cup M_i$\;
	}
	\For{jede Teilmenge $P' \in M$}{
		$S \leftarrow S \cup z(P')$\;
		$w(z(P')) \leftarrow \left|P'\right|$\;
		$\Delta \leftarrow \Delta + \sum_{x \in P'} \EuclidSquared{x}{z(P')}$\;
	}
	\Return{$S, w, \Delta$}
\end{algorithm}
Sobald bei allen Teilmengen die Partitionierung abgebrochen ist, ersetzen wir wieder jede Teilmenge durch ihren Zentroiden. Die
Vereinigung aller Zentroiden entspricht dann wieder den Punkten der Kernmenge. Die Gewichtung $w$ und die Verschiebung $\Delta$
werden ebenfalls analog berechnet.
\absatz
Algorithmus~\ref{algo:coreset} ist die Pseudocode-Beschreibung unserer Variante der Kernmengenkonstruktion. In den Zeilen
5-6 wird die initiale Partitionierung gemäß $D^2$-Gewichtung vorgenommen. Anschließend führen wir in den Zeilen 7-9 die
rekursive Partitionierung für jede der Teilmengen solange durch, bis entweder die maximale Tiefe erreicht wurde, oder die
Abbruchbedingung über die Kosten einer weiteren Partitionierung erreicht ist. Wir vereinigen alle berechneten Teilmenge zur
Menge $M$. In den Zeilen 10-14 gehen wir analog zur ursprünglichen Konstruktion vor. Alle Teilmengen werden wieder durch ihren
Zentroiden ersetzt.

\begin{algorithm}
\caption{\texttt{Partition2}}
\label{algo:partition}
	% Optionen
	\DontPrintSemicolon
	\SetKw{KwOr}{\textbf{or}}
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}, d, f, t, T$}
	\Output{Partitionierung $M$}
	\BlankLine
	
	% Algo
	$m \leftarrow d \cdot k$\;
	$M \leftarrow \emptyset$\;
	Wähle $m$ Zentren $C^t = \{c_1, \dots, c_m\}$ gemäß $D^2$-Gewichtung aus $P$\;
	Sei $P_1, \dots, P_m$ die durch $C^t$ induzierte Partitionierung von $P$\;
	Sei $z(P)$ der Zentroid von $P$\;
	\If{$t=T$ \KwOr $\sum_{x \in P} \EuclidSquared{x}{z(P)} \leq \frac{1}{f} \sum_{i=1}^{m} \sum_{y \in P_i} \EuclidSquared{y}{z(P_i)} $}{
		$M \leftarrow M \cup P$\;	
	}\Else{
		\For{$i \leftarrow 1$ \KwTo $m$}{
			$M \leftarrow M \cup $ \texttt{Partition2}($P_i,k,d,f,t+1,T$)\;
		}
	}
	\Return{$M$}
\end{algorithm}
Algorithmus~\ref{algo:partition} ist unsere Variante der Partitionierung. In den Zeilen 3-5 wählen wir $d \cdot k$ Zentren
gemäß $D^2$-Gewichtung und prüfen in Zeile 6 die modifizierte Abbruchbedingung. Ansonsten ist die Partitionierung unverändert.
\absatz
Wir können bei unserer Variante der Kernmengenkonstruktion keine Garantien für die Laufzeit oder die Qualität der berechneten
Kernmenge mehr geben. Es ist daher unerlässlich, die Konstruktion empirisch auszuwerten. Dies wollen wir in
Abschnitt~\ref{subsection:experiment-coreset} des nächsten Kapitels tun.