\section{Kernel-\texorpdfstring{$k$}{k}-means Methoden für spektrales Graphclustering}
\label{section:main}

In diesem Kapitel wollen wir Verfahren zur spektralen Clusteranalyse von Graphen vorstellen und verbessern. Als Grundlage wollen
wir den Kernel-$k$-means Algorithmus verwenden. Lloyds Algorithmus ist nach wie vor eine Heuristik, die in der Praxis mit geringen
Ausführungszeiten Clusterings von akzeptabler Qualität berechnet. Unter Einsatz der Kernel Methode können wir die
Qualität der berechneten Clusterings noch einmal steigern, ohne dabei signifikante Performanzeinbußen in Kauf nehmen zu müssen.
Der Algorithmus ist zudem leicht zu implementieren. Gerade im Bereich der Clusteranalyse sollte dies nicht unterschätzt werden,
da eine praktische Evaluation der Algorithmen in diesem anwendungsreichen Gebiet unerlässlich ist.
\absatz
Das Kapitel ist folgendermaßen gegliedert: In Abschnitt~\ref{subsection:wkkm-graphcut-graphclustering} diskutieren wir, wie wir
mit dem Kernel-$k$-means Algorithmus Graphclusteringprobleme (und umgekehrt) lösen können. In Abschnitt~\ref{subsection:kernelkmpp}
zeigen wir, wie sich die Kernel Methode auch auf den Algorithmus \kmpp{} anwenden lässt und wie wir damit den
Kernel-$k$-means Algorithmus für Graphclustering verbessern können.

\subsection{Graphschnitte und Graphclustering mit gewichtetem Kernel-\texorpdfstring{$k$}{k}-means}
\label{subsection:wkkm-graphcut-graphclustering}

Um die Graphclusteringprobleme aus Definition~\ref{def:graphpartitioning} mit dem Kernel-$k$-means Algorithmus zu lösen,
müssen wir uns zunächst überlegen, wie wir diese in eine Eingabe für den Algorithmus transformieren können. Dass eine solche
Transformation (bidirektional) möglich ist, haben bereits Dhillon, Guan und Kulis gezeigt~\cite{DhillonGK04,DhillonGK07}. Wir
fassen daher zunächst den für uns relevanten Teil ihrer Ergebnisse zusammen.
\absatz
Die grundlegende Idee besteht darin, zu zeigen, dass sowohl die Zielfunktion für Kernel-$k$-means als auch die (diversen)
Zielfunktionen für Graphpartitionierungsprobleme als \emph{Spurmaximierungsprobleme} formuliert werden können. Die
\emph{Spur} (engl. \emph{trace}) einer quadratischen $n \times n$-Matrix $M$ ist dabei die Summe der Elemente auf der
Hauptdiagonalen von $M$:
\[ \tr{M} = \sum_{i=1}^{n} M_{i,i} \]
Weiterhin gilt für zwei quadratische Matrizen $M,M'$:
\begin{enumerate}
	\item 	$\tr{MM'} = \tr{M'M}$
	\item 	$\tr{M^TM} = \frobenius{M}^2$
	\item 	$\tr{M+M'} = \tr{M} + \tr{M'}$
\end{enumerate}
Wir formen zunächst die Kernel-$k$-means Zielfunktion um und widmen uns dann den Graphpartitionierungsproblemen.

\paragraph{Gewichtetes Kernel-$k$-means.} Für die Umformung des $k$-means-Problems legen wir das \emph{gewichtete}
Kernel-$k$-means Problem (engl. \emph{weighted kernel $k$-means}, kurz WKKM) zu Grunde.
Dieses lautet analog zu Definition~\ref{def:kmeans-kmedian-innerproduct} wie folgt.
Unsere Eingabe besteht aus der gewichteten Punktmenge
als $n \times d$-Matrix $A$, einer Gewichtsfunktion $w$, die einen Punkt-Zeilenvektor $\vect{p}$ aus $A$ auf ein reelles Gewicht
abbildet, und der Clusteranzahl $k$. Die Zielfunktion besteht wie gewohnt darin, die quadrierten euklidischen Distanzen der
Punkte eines Clusters zu ihrem Clusterzentrum zu minimieren:
\begin{align}
\label{formula:wkkm-objective}
	\min D(\{S_i\}_{i=1}^{k}) = \sum_{i=1}^{k} \sum_{\vect{p} \in S_i} w(\vect{p}) \EuclidSquared{\phi(\vect{p})}{\vect{c}_i}
\end{align}
Dabei sind die $S_1, \dots, S_k$ die Cluster, wobei ein Cluster $S_i$ das Zentrum
\[ \vect{c}_i = \frac{\sum_{\vect{p} \in S_i} w(\vect{p}) \phi(\vect{p})}{\sum_{\vect{p} \in S_i} w(\vect{p})} \]
hat. Wir setzen auch hier wieder den Kernel-Trick für die Distanzberechnungen ein. Dazu berechnen wir initial $n^2$ viele
Skalarprodukte $\innerproduct{\phi(\vect{p}_i)}{\phi(\vect{p}_j)}$ und speichern diese in einer
$n \times n$-Kernelmatrix $K$ mit $K_{i,j} = \kappa(\vect{p}_i, \vect{p}_j) = \innerproduct{\phi(\vect{p}_i)}{\phi(\vect{p}_j)}$.
Die Distanzberechnung von einem Punkt zu seinem Clusterzentrum
können wir ebenfalls analog zu~\ref{formula:kernel-kmeans} mit Hilfe der Kernelmatrix umformulieren. Sie unterscheidet sich von
dieser nur darin, dass die Punkte gewichtet sind:
\begin{align}
\label{formula:wkkm}
	\EuclidSquared{\phi(\vect{p}_j)}{\vect{c}_i} =
	K_{i,i} - \frac{2 \sum_{\vect{p}_j \in S_i} w(\vect{p}_j) K_{i,j}}{\sum_{\vect{p}_j \in S_i} w(\vect{p}_j)}
	+ \frac{\sum_{\vect{p}_j, \vect{p}_l \in S_i} w(\vect{p}_j) w(\vect{p}_l) K_{j,l}}{\left(\sum_{\vect{p}_j \in S_i} w(\vect{p}_j)\right)^2}
\end{align}
Die Erweiterung von Algorithmus~\ref{algo:kernel-lloyd} zu Algorithmus~\ref{algo:wkkm} ergibt sich ebenfalls unmittelbar.
\begin{algorithm}
\caption{Weighted Kernel-$k$-means}
\label{algo:wkkm}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	\SetKw{KwGoTo}{go to}
	
	% Input/Output
	\Input{Kernel-Matrix $K$, $k \in \mathbb{N}^{+}$, Punktgewichte $w$}
	\Output{$k$-means-Clustering der Eingabepunktmenge}
	\BlankLine
	
	% Algo
	Wähle $k$ initiale Cluster $S_1^{(0)}, \dots, S_k^{(0)}$;
	$t \leftarrow 0$\;
	\ForEach{$\vect{p}$}{\label{iter}
		Bestimme $c^*(\vect{p}) \leftarrow \argmin_{i=1}^{k} \EuclidSquared{\phi(\vect{p})}{\vect{c_i}}$ mit~\ref{formula:wkkm}\;
	}
	\For{$i \leftarrow 1$ \KwTo $k$}{
		$S_i^{(t+1)} \leftarrow \{ \vect{p} : c^*(\vect{p}) = i \}$\;
	}
	$t \leftarrow t+1$\;
	\If{$t < t_{max}$ \textbf{and} nicht konvergiert}{
		\KwGoTo \ref{iter}\;
	}\Else{
		\Return{$S_1^{(t)}, \dots, S_k^{(t)}$}
	}
\end{algorithm}
\paragraph{Gewichtetes Kernel-$k$-means als Spurmaximierung.}
Wir formen als nächstes die Zielfunktion des gewichteten Kernel-$k$-means Problems in ein Spurmaximierungsproblem um.
Dazu gehen wir zunächst davon aus, dass die Punktgewichte in einer $n \times n$-Diagonalmatrix $W$ gespeichert sind, sodass
$W_{l,l} = w(\vect{p}_j)$ für alle Punkte $\vect{p}_j$ und die Punktgewichte des Clusters $S_i$ in der Diagonalmatrix
$W^i$ mit $W_{m,m}^j = w(\vect{p}_m)$ für alle Punkte $\vect{p}_m \in S_i$ gespeichert sind. 
\absatz
Sei nun $s_i$ die Summe der Gewichte der Punkte in Cluster $i$, also
\[ s_i = \sum_{\vect{p}_j \in S_i} w(\vect{p}_j) = \sum_{\vect{p}_j \in S_i} W_{j,j}^i \]
Wir betrachten die $n \times k$-Matrix $Z$ mit den Einträgen
\[ Z_{j, i} = 	\begin{cases}
					\frac{1}{\sqrt{s_i}}, & \textrm{ falls } \vect{p}_j \in S_i, \\
					0 & \textrm{ sonst }
				\end{cases} \]
sowie die (nicht explizit berechnete) Matrix $\Phi_i = [\phi(\vect{p}_1), \dots, \phi(\vect{p}_m)]$ von abgebildeten Punkten
des Clusters $S_i = \{ \vect{p}_1, \dots, \vect{p}_m \}$. Mit diesen können wir
die Zielfunktion des gewichteten Kernel-$k$-means Problems~(\ref{formula:wkkm-objective}) umformen. Wir erinnern uns, dass diese
die Minimierung der Summe der Intraclusterabstände ist. Notieren wir den Intraclusterabstand eines Clusters $S_i$ mit
$d(S_i) = \sum_{\vect{p} \in S_i} w(\vect{p}) \EuclidSquared{\phi(\vect{p})}{\vect{c}_i}$, können wir für den Zielfunktionswert
$D(\{S_i\}_{i=1}^{k})$ verkürzt schreiben:
\[ D(\{S_i\}_{i=1}^{k}) = \sum_{i=1}^k d(S_i) \]
Der Zentroidvektor $c_i$ des Clusters $S_i$ berechnet sich in unserer ursprünglichen Problemformulierung durch
\[ \vect{c}_i = \frac{\sum_{\vect{p} \in S_i} w(\vect{p}) \phi(\vect{p})}{\sum_{\vect{p} \in S_i} w(\vect{p})} \]
Diese Berechnungsvorschrift ist äquivalent zu
\[ \vect{c}_i = \Phi_i \frac{W_j \vect{e}}{s_j}, \]
wobei $\vect{e}$ der Einheitsvektor der passenden Dimension ist.
Wir können alternativ unter Einsatz der Matrix $Z$ auch direkt eine Matrix $C$ aller Clusterzentren bestimmen,
wenn wir für die gesamte Eingabepunktmenge die (ebenfalls nicht explizit berechnete) Abbildungsmatrix
$\Phi = [\Phi_1, \dots, \Phi_k] = [\phi(\vect{p}_1), \dots, \phi(\vect{p}_n)]$ verwenden:
\[ C = \Phi W Z Z^T \]
In diesem Fall entspricht das Zentrum eines einzelnen Clusters $S_i$
\[ \vect{c}_i = \left(\Phi W Z Z^T\right)_{\cdot i} \]
wobei wir für eine Matrix $M$ mit $M_{\cdot i}$ die $i$-te Spalte von $M$ meinen. Damit ist es uns nun möglich, den
Zielfunktionswert so zu formulieren, dass wir später eine Äquivalenz zur Graphpartitionierung sehen werden. Mit
$Y = W^{\frac{1}{2}} Z$ gilt:
\begin{align*}
	D(\{S_i\}_{i=1}^{k}) &= \sum_{i=1}^{k} \sum_{\vect{p} \in S_i} w(\vect{p}) \EuclidSquared{\phi(\vect{p})}{\vect{c}_i} \\
		&= \sum_{j=1}^{n} W_{j,j} \bigg\lVert \Phi_{\cdot j} - \left( \Phi W Z Z^T \right)_{\cdot j} \bigg\rVert^2 \\
		&= \sum_{j=1}^{n} W_{j,j} \bigg\lVert \Phi_{\cdot j} - \left( \Phi W^{\frac{1}{2}} Y Y^T W^{-\frac{1}{2}} \right)_{\cdot j} \bigg\rVert^2 \\
		&= \sum_{j=1}^{n} \bigg\lVert \Phi_{\cdot j} \left(W_{j,j}\right)^{\frac{1}{2}} - \left( \Phi W^{\frac{1}{2}} Y Y^T \right)_{\cdot j} \bigg\rVert^2 \\
		&= \big\lVert \Phi W^{\frac{1}{2}} - \Phi W^{\frac{1}{2}} Y Y^T \big\rVert_F^2
\end{align*}
Der ursprüngliche Zielfunktionswert ist damit auf die (quadrierte) Frobenius-Norm von
$\Phi W^{\frac{1}{2}} - \Phi W^{\frac{1}{2}} Y Y^T$ transformiert.
Wir können nun die oben genannte Verbindung zwischen der Frobenius-Norm und der Matrix-Spur ausnutzen, um zur beabsichtigten
Spur-Maximierung zu gelangen. Wir erinnern uns, dass für eine Matrix $M$ gilt, dass $\tr{M^TM} = \frobenius{M}^2$. Wir können
somit weiter umformen:
\begin{align*}
	D(\{S_i\}_{i=1}^{k}) &= \tr{ W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} - W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} Y Y^T \\
		& \; \; \; \; - Y Y^T W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} + Y Y^T W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} Y Y^T } \\
		&= \tr{ W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} } - \tr{ Y^T W^{\frac{1}{2}} \Phi^T \Phi W^{\frac{1}{2}} Y }
\end{align*}
In der Kernel-Matrix $K$ speichern wir sämtliche Skalarprodukte der projizierten Punkte, demnach ist per Definition
$K = \Phi^T \Phi$.
Die Kernel-Matrix ist genau wie die Gewichts-Matrix $W$ konstant, da beide Teil der Eingabe sind. Dementsprechend ist auch
$\tr{ W^{\frac{1}{2}} K W^{\frac{1}{2}} }$ eine Konstante. Die Minimierungszielfunktion~\ref{formula:wkkm-objective} ist
daher gleichzusetzen mit der Maximierungszielfunktion des folgenden Spurmaximierungsproblems:
\begin{align}
\label{formula:wkkm-tracemax}
	\max_{Y}{} \tr{ Y^T W^{\frac{1}{2}} K W^{\frac{1}{2}} Y }
\end{align}
Da das eigentliche Clustering von Algorithmus~\ref{algo:wkkm} offenbar einer spurmaximierenden Wahl der Matrix $Y$ gleichkommt,
wollen wir abschließend noch kurz einen genaueren Blick auf diese werfen.
Die Matrix $Y = W^{\frac{1}{2}} Z$ ist eine orthonormale Diagonalmatrix, das heißt $Y^T Y = I$. Ihre Einträge sehen
dementsprechend wie folgt aus:
\[ Y = 	\begin{pmatrix}
			\frac{W_1^{\frac{1}{2}} \vect{e}}{\sqrt{s_1}} & & & \\
			& \frac{W_2^{\frac{1}{2}} \vect{e}}{\sqrt{s_2}} & & \\
			& & \dots & \\
			& & & \frac{W_k^{\frac{1}{2}} \vect{e}}{\sqrt{s_k}}
		\end{pmatrix} \]

\paragraph{Graphpartitionierung als Spurmaximierung.}
Es ist möglich, für alle in Definition~\ref{def:graphpartitioning} vorgestellten Problemstellungen ein äquivalentes
Spurmaximierungsproblem anzugeben. Wir beschränken uns hier zunächst auf die beiden Zielfunktionen mit praktischer Relevanz
für das Evaluations-Kapitel dieser Arbeit, nämlich die Ratio Association und den Normalized Cut. Wir beginnen mit der
Ratio Association.

Wir führen zunächst einen Indikator-Vektor $\vect{x}_c$ der Dimension $n$ für jede Partition beziehungsweise jedes Cluster $c$
ein, sodass $\vect{x}_c(i) = 1$ genau dann, wenn das Cluster $c$ den Knoten $i$ enthält. Die Zielfunktion bei der Ratio
Association lautet
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|} \]
Wir nehmen an, dass $A$ die (gewichtete) Adjazenzmatrix des Eingabegraphen ist und definieren zusätzlich
\[ \tilde{\vect{x}}_c = \frac{\vect{x}_c}{(\vect{x}_c^T \vect{x}_c)^\frac{1}{2}} \]
Das Produkt $\vect{x}_c^T \vect{x}_c$ entspricht der Größe der Partition $c$:
\[ \left|V_c\right| = \vect{x}_c^T \vect{x}_c \]
Die Summe der Kantengewichte der Partition $w(V_c, V_c)$ können wir über folgendes Produkt abbilden:
\[ w(V_c, V_c) = \vect{x}_c^T A \vect{x}_c \]
Damit können wir eine erste Transformation der Zielfunktion vornehmen:
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{\left|V_c\right|}
	\Leftrightarrow \max \sum_{c=1}^{k} \frac{\vect{x}_c^T A \vect{x}_c}{\vect{x}_c^T \vect{x}_c}
	\Leftrightarrow \max \sum_{c=1}^{k} \tilde{\vect{x}}_c^T A \tilde{\vect{x}}_c \]
Betrachten wir $\tilde{X}$ als Matrix, deren $c$-te Spalte der Vektor $\tilde{\vect{x}}_c$ ist, können wir die Zielfunktion
umformen zu
\begin{align}
\label{formula:ratioassoc-tracemax}
	\max_{\tilde{X}}{} \tr{ \tilde{X}^T A \tilde{X} }
\end{align}
Wenn wir in~\ref{formula:wkkm-tracemax} für die Gewichtsmatrix $W$ die Einheitsmatrix wählen, also $W = \mathbbm{1}_n$ und
die Adjazenzmatrix als Kernel-Matrix setzen, sodass $K = A$, dann sind~\ref{formula:ratioassoc-tracemax}
und~\ref{formula:wkkm-tracemax} äquivalent. Auf diese Weise können wir das Ratio Association Problem mit unserem Algorithmus
für gewichtetest Kernel-$k$-means lösen.
\absatz
Für den Normalized Cut gehen wir ähnlich vor. Unsere ursprüngliche Zielfunktion lautet
\[ \min \sum_{c=1}^{k} \frac{w(V_c, V \setminus V_c)}{w(V_c, V)} \]
Zunächst wollen wir die Funktion in ein Maximierungsproblem umwandeln. Wir beobachten, dass die Zielfunktion äquivalent ist
zu
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{w(V_c, V)} \]
Wir benötigen zur Umformung des Normalized Cut die diagonale Gradmatrix $D$ des Graphen, welcher der Adjazenzmatrix $A$ zu
Grunde liegt. Wir verwenden wieder die Indikator-Vektoren und definieren zusätzlich
\[ \hat{\vect{x}}_c = \frac{\vect{x}_c}{ (\vect{x}_c^T D \vect{x}_c)^{\frac{1}{2}} } \]
Unter Verwendung der Gradmatrix können wir die Variante des Normalized Cut als Maximierungsproblem wie folgt umformen:
\[ \max \sum_{c=1}^{k} \frac{w(V_c, V_c)}{w(V_c, V)}
	\Leftrightarrow \max \sum_{c=1}^{k} \frac{\vect{x}_c^T A \vect{x}_c}{\vect{x}_c^T D \vect{x}_c} 
	\Leftrightarrow \max \sum_{c=1}^{k} \hat{\vect{x}}_c^T A \hat{\vect{x}}_c \]
Wir nehmen analog zur Ratio Association an, dass die Matrix $\hat{X}$ die Spaltenmatrix der Vektoren $\hat{\vect{x}}_c$ ist.
Mit $\tilde{Y} = D^\frac{1}{2} \hat{X}$ lautet das Spurmaximierungsproblem für den Normalized Cut folgendermaßen:
\begin{align}
\label{formula:ncut-tracemax}
	\max_{\tilde{Y}}{} \tr{ \tilde{Y}^T D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \tilde{Y} }
\end{align}
Das Problem in~\ref{formula:ncut-tracemax} ist äquivalent zu~\ref{formula:wkkm-tracemax}, wenn wir als
Gewichtsmatrix $W = D$ und als Kernel-Matrix $K = D^{-1} A D^{-1}$ setzen.
\absatz
Wir wollen abschließend noch anmerken, dass die Adjazenzmatrix $A$ positiv definit sein muss, damit sie als Kernel-Matrix in
Algorithmus~\ref{algo:wkkm} garantiert zu einer iterativen Verringerung des gewichteten Kernel-$k$-means Zielfunktionswertes
führt. Um dies in der Praxis sicherzustellen, ist bei der Wahl der Kernel-Matrix gegebenenfalls das Aufaddieren einer
"`Verschiebung"' auf die ursprüngliche Kernel-Matrix nötig. Wir gehen auf dieses Detail im Experimente-Kapitel noch genauer ein.

\subsection{Die Kernel Methode für \texorpdfstring{$k$}{k}-means\texttt{++}}
\label{subsection:kernelkmpp}

Ein wichtiges Merkmal von Algorithmus~\ref{algo:wkkm} ist, dass er grundsätzlich ohne Methoden der spektralen Clusteranalyse
auskommt. Wir müssen uns jedoch wie auch bei Lloyds Algorithmus ohne Kernel Methode mit der geeigneten initialen Wahl
der Clusterzentren beschäftigen. Zunächst einmal wählen wir diese wie schon zuvor zufällig aus der Eingabepunktmenge.
Diese Vorgehensweise ist durchaus praktikabel, die experimentelle Evaluation aus~\citep{DhillonGK04,DhillonGK07} zeigt jedoch,
dass sich mit einer geschickteren Wahl die Qualität des letztlich berechneten Clusterings noch signifikant steigern lässt.
Die Autoren verwenden für die Initialisierung die spektrale Clusteranalyse, was wiederum den Aufwand der Eigenvektor-Berechnung
mit sich bringt.
\absatz
Konkret wird mit einem Verfahren aus~\cite{GolubL96} eine Problemrelaxion
von~\ref{formula:wkkm-tracemax}/~\ref{formula:ratioassoc-tracemax} beziehungsweise
von~\ref{formula:wkkm-tracemax}/~\ref{formula:ncut-tracemax} gelöst.
Relaxieren wir die Wahl von $Y$ beziehungsweise $\tilde{Y}$ auf eine beliebige
orthonormale Matrix, dann sind diese von der Form $Y = V_k Q$ beziehungsweise $\tilde{Y} = \tilde{V}_k Q$.
Dabei ist $Q$ eine beliebige orthogonale $k \times k$-Matrix, $V_k$ die Matrix der $k$ größten Eigenvektoren von
$W^{\frac{1}{2}} K W^{\frac{1}{2}}$ und $\tilde{V}_k$ die Matrix der $k$ größten Eigenvektoren von
$D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$.

Die Berechnung der jeweiligen $k$ größten Eigenvektoren wird mit wachsender Clusteranzahl
$k$ aufwändig, wie wir schon in Abschnitt~\ref{subsection:basics:kernel-spectral} beobachtet haben.
Intuitiv ist daher die Fragestellung, ob es möglich ist, eine Initialisierung ohne Eigenvektor-Berechnungen vorzunehmen,
die dennoch deutlich verbesserte Clusterings mit dem Kernel-$k$-means-Algorithmus ermöglicht.

In Abschnitt~\ref{subsection:basics:clustering} haben wir bereits den Algorithmus \kmpp{} von Arthur und
Vassilvitskii~\cite{ArthurV07} kennengelernt. Die initiale Wahl der Cluster erfolgt hier gemäß einer
Wahrscheinlichkeitsverteilung der Punkte als mögliche Clusterzentren, die proportional zur quadrierten euklidischen Distanz
zum jeweils nächstgelegenen bereits gewählten Zentrum ist. Wir zeigen nun, dass die Kernel Methode auch auf den Algorithmus
\kmpp{} angewendet werden kann. Damit ist es uns möglich, Algorithmus~\ref{algo:kernel-lloyd} (und damit
auch Algorithmus~\ref{algo:wkkm}) in eine Kernel-basierte Variante von \kmpp{} umzuwandeln, die wir \kkmpp{} nennen.
\absatz
Wir erinnern uns, dass wir im Zusammenhang mit \kmpp{} für einen Punkt $x$ aus der Eingabe-Punktmenge $P$ mit $D(x)$
die geringste Distanz von $x$ zum nächstgelegenen bereits gewählten Zentrum notieren. Der Punkt $x$ wird als nächstes
Zentrum mit der Wahrscheinlichkeit
\[ \frac{D(x)^2}{\sum_{y \in P} D(y)^2} \]
gewählt. Wir können demnach die Kernel Methode auf den Algorithmus \kmpp{} anwenden, wenn wir für jeden Eingabepunkt $x$ den
Wert $D(\phi(x))$ beziehungsweise $D(\phi(x))^2$ berechnen können. Dazu zeigen wir nun, dass wir diese Werte wie schon zuvor
beim Kernel-$k$-means-Algorithmus ausschließlich über die Kernelfunktion $\kappa$ ohne explizite Kenntnis der abgebildeten
Punkte oder der Abbildung $\phi$ selbst berechnen können.

Wir nehmen zunächst an, dass die Zentren $c_1, \dots, c_l$ mit $l \in \{ 1, \dots, k-1 \}$ bereits gewählt worden sind.
Dann können wir $D(x)$ formal notieren als das Minimum über alle quadrierten euklidischen Distanzen von $x$ zu jedem der bisher
gewählten Zentren $c_i$:
\[ D(x) = \min_{i = 1 \dots l} \EuclidSquared{x}{c_i} \]
Wenden wir die Kernel Methode an, berechnet sich $D(\phi(x))$ dementsprechend folgendermaßen:
\[ D(\phi(x)) = \min_{i = 1 \dots l} \EuclidSquared{\phi(x)}{\phi(c_i)} \]
Demnach genügt es, zu zeigen, dass wir $\EuclidSquared{\phi(x)}{\phi(c_i)}$ ausschließlich mit der Kernelfunktion berechnen
können. Wir haben bereits gesehen, dass die Kernelfunktion gerade dafür gedacht ist. Es gilt:
\begin{align}
	\EuclidSquared{\phi(x)}{\phi(c_i)} = \kappa(x,x) - 2 \kappa(x,c_i) + \kappa(c_i,c_i)
\end{align}
Mit denselben Überlegungen können wir auch zeigen, dass sich der Wert von $D(\phi(x))^2$ mit der Kernel Methode berechnen lässt.
Zunächst lässt sich auch $D(\phi(x))^2$ als Minimum formulieren:
\[ D(\phi(x))^2 = \left( \min_{i = 1 \dots l} \EuclidSquared{\phi(x)}{\phi(c_i)} \right)^2 \]
Dazu müssen wir effektiv $\left( \EuclidSquared{\phi(x)}{\phi(c_i)} \right)^2$ berechnen. Auch dies lässt sich mit der
Kernelfunktion umsetzen:
\begin{align}
\label{formula:kkmpp}
	\left( \EuclidSquared{\phi(x)}{\phi(c_i)} \right)^2 &= \left( \kappa(x,x) - 2 \kappa(x,c_i) + \kappa(c_i,c_i) \right)^2 \nonumber \\
	&= \left(\kappa(x,x)\right)^2 - 4 \kappa(x,x) \kappa(x,c_i) + 2 \kappa(x,x) \kappa(c_i,c_i) \nonumber \\
	& \; \; \; + 4 \kappa(x,c_i) \kappa(x,c_i) - 4 \kappa(x,c_i) \kappa(c_i,c_i) + \left(\kappa(c_i,c_i)\right)^2
\end{align}
Der Algorithmus \kkmpp{} wird damit die logische Weiterentwicklung von \kmpp{} auf Basis der Kernel Methode.

\begin{algorithm}
\label{algo:kernelkmeanspp}
\caption{\kkmpp}
	% Optionen
	\DontPrintSemicolon
	\SetKwInOut{Input}{Eingabe:}
	\SetKwInOut{Output}{Ausgabe:}
	
	% Input/Output
	\Input{$P \subseteq \Rd, k \in \mathbb{N}^{+}$, Kernelfunktion $\kappa$}
	\Output{$k$ initiale Clusterzentren für $P$}
	\BlankLine
	
	% Algo
	Wähle $c_1$ zufällig gleichverteilt aus $P$\;
	\For{$i \leftarrow 2$ \KwTo $k$}{
		Berechne $D(\phi(x'))^2$ mit~\ref{formula:kkmpp} für alle $x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$\;
		Wähle den Punkt $x' \in P \setminus \{ c_1, \dots, c_{i-1} \}$ als Zentrum $c_i$ mit Wahrscheinlichkeit $\frac{D(\phi(x'))^2}{\sum_{x \in P} D(\phi(x))^2}$\;
	}
	Führe den Kernel-$k$-means-Algorithmus mit den initialen Clusterzentren $c_1, \dots, c_k$ aus.
\end{algorithm}
Wir zeigen nun, dass auch \kkmpp{} eine $\BigO{\log k}$-Approximation für das $k$-means-Problem ist. Strukturell gehen
wir dabei analog zu dem Papier von Arthur und Vassilvitskii~\cite{ArthurV07} vor, da sich durch die Kernel Methode keine
wesentlichen Änderungen ergeben. Für ein Clustering $C = \{ C_1, \dots, C_k \}$ mit den jeweiligen Clusterzentren
$c_1, \dots, c_k$ bezeichnen wir mit dem \emph{Potenzial} $\theta$ den Zielfunktionswert des Clusterings bezüglich der
$k$-means-Zielfunktion, das heißt
\[ \theta(C) = \sum_{i=1}^k \theta(C_i) = \sum_{i=1}^k \sum_{p \in C_i} \EuclidSquared{p}{c_i} \]
Das Potenzial des optimalen Clusterings $C_\textrm{OPT}$ notieren wir mit $\optpot$. Bevor wir mit der Analyse beginnen,
benötigen wir folgendes Lemma, das einen beliebigen Punkt mit einer Punktmenge und ihrem Zentroiden in Relation setzt:

\begin{lemma}
\label{lemma:point-centroid}
	Sei $S$ eine Punktmenge mit dem geometrischen Zentroiden $z(S)$. Weiterhin sei $\vect{q}$ ein beliebiger Punkt. Es gilt:
	\[ \sum_{\vect{x} \in S} \EuclidSquared{\vect{x}}{\vect{q}} - \sum_{\vect{x} \in S} \EuclidSquared{\vect{x}}{z(S)}
		= \left|S\right| \cdot \EuclidSquared{z(S)}{\vect{q}} \]
\end{lemma}
Wir zeigen nun das eigentliche Ergebnis.

\begin{satz}[\kkmpp{} ist $\BigO{\log k}$-kompetitiv]
\label{thm:kkmpp-ologk-approx}
	Wenn das Clustering $C$ mit \kkmpp{} berechnet wird, dann gilt für das Potenzial $\theta$ von $C$:
	\[ E[\theta(C)] \leq 8 (\ln{k} + 2) \optpot \]
\end{satz}
Der Beweis erfolgt in drei Schritten. Wir zeigen zunächst, dass das erste zufällig gleichverteilt gewählte Clusterzentrum
die Approximationseigenschaft erfüllt. Dieselbe Eigenschaft zeigen wir im zweiten Schritt für die übrigen Zentren, die gemäß
der $D^2$-Gewichtung gewählt werden. Im dritten und letzten Schritt beweisen wir mittels Induktion das Gesamtergebnis.

\begin{lemma}[Das erste Zentrum ist kompetitiv]
\label{lemma:kkmpp-first-center-approx}
	Sei $A$ ein beliebiges Cluster von $C_\textrm{OPT}$ und sei $C$ ein Clustering mit genau einem Zentrum, das zufällig
	gleichverteilt aus $A$ gewählt wird. Dann gilt:
	\[ E[\theta(A)] = 2 \optpot(A) \]
\end{lemma}
\begin{proof}
	Sei $z(A)$ der geometrische Zentroid der Punkte in $A$, das heißt
	\[ z(A) = \frac{1}{\left|A\right|} \sum_{\vect{p} \in A} \vect{p} \]
	Mit Lemma~\ref{lemma:point-centroid} gilt:
	\begin{align*}
		& \sum_{\vect{p} \in A} \frac{1}{\left|A\right|} \cdot \left( \sum_{\vect{p}' \in A} \EuclidSquared{\vect{p}}{\vect{p}'} \right) \\
		&= \frac{1}{\left|A\right|} \sum_{\vect{p} \in A} \left( \sum_{\vect{p}' \in A} \EuclidSquared{\vect{p}'}{z(A)} 
			+ \left|A\right| \cdot \EuclidSquared{\vect{p}}{z(A)} \right) \\
		&= 2 \sum_{\vect{p} \in A} \EuclidSquared{\vect{p}}{z(A)} \\
		&= E[\theta(A)]
	\end{align*}
\end{proof}
Wir zeigen nun, dass die Approximationseigenschaft auch für die übrigen Zentren, die gemäß der $D^2$-Gewichtung gewählt werden,
gilt.
\begin{lemma}[Die übrigen Zentren sind kompetitiv]
\label{lemma:kkmpp-remaining-center-approx}
	Sei $A$ ein beliebiges Cluster von $C_\textrm{OPT}$ und sei $C$ ein beliebiges Clustering. Wenn ein Zentrum aus $A$
	zufällig nach der $D^2$-Gewichtung zu $C$ hinzugefügt wird, dann gilt:
	\[ E[\theta(A)] \leq 8 \optpot(A) \]
\end{lemma}
\begin{proof}
	Ein Punkt $\vect{p} \in A$ wird mit der Wahrscheinlichkeit
	\[ \frac{D(\phi(\vect{p}))^2}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2} \]
	als Zentrum gewählt. Wenn der Punkt $\vect{p}$ als Zentrum gewählt wird, beträgt der Potenzial-Wert eines Punktes $\vect{p'}$,
	der dem Zentrum $\vect{p}$ zugeordnet ist, genau
	\[ \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \]
	Der Erwartungswert für das Potenzial des gesamten Clusters $A$ ist dementsprechend:
	\[  E[\theta(A)] = \sum_{\vect{p} \in A} \frac{D(\phi(\vect{p}))^2}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2}
		\sum_{\vect{p}' \in A} \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \]
	Für die Berechnung von $D(\phi(\vect{p}'))$ und der Distanzen $\Euclid{\phi(\vect{p}')}{\phi(\vect{p})}$ verwenden
	wir im \kkmpp-Algorithmus die Kernelfunktion $\kappa$. Wenn die Kernelfunktion, beziehungsweise die zugehörige
	Kernelmatrix \emph{positiv definit} ist (für Details siehe beispielsweise~\cite{Shawe-TaylorC04}), gilt für diese die
	Cauchy-Schwarz-Ungleichung, also $\left|\kappa(x,y)\right|^2 \leq \kappa(x,x) \cdot \kappa(y,y)$.
	Aus der Cauchy-Schwarz-Ungleichung folgen die Dreiecks-Ungleichung, sowie die Power-Mean-Ungleichung bezüglich der
	Kernelfunktion. Die üblichen Kernelfunktionen wie beispielsweise in Tabelle~\ref{tbl:kernel-functions} sowie die
	in unseren Experimenten eingesetzten Kernel haben eine positiv definite zugehörige Kernelmatrix. Wir können daher an
	dieser Stelle die Dreiecks-Ungleichung anwenden und erhalten für alle $\vect{p}, \vect{p}'$:
	\[ D(\phi(\vect{p})) \leq D(\phi(\vect{p}')) + \Euclid{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Für die $D^2$-Gewichtung gilt zudem mit der Power-Mean-Ungleichung:
	\[ D(\phi(\vect{p}))^2 \leq 2 D(\phi(\vect{p}'))^2 + 2 \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Summiert über alle Punkte $\vect{p} \in A$ erhalten wir somit:
	\[ D(\phi(\vect{p}))^2 \leq \frac{2}{\left|A\right|} \sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2
		+ \frac{2}{\left|A\right|} \sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Wir schätzen nun $E[\theta(A)]$ nach oben ab:
	\begin{align*}
		E[\theta(A)] 	&\leq \frac{2}{\left|A\right|} \cdot \sum_{\vect{p} \in A} \frac{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2}
								\cdot \sum_{\vect{p}' \in A} \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \\
						& \; \; \; \; \; + \frac{2}{\left|A\right|} \cdot
							\sum_{\vect{p} \in A} \frac{\sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})}}{\sum_{\vect{p}' \in A} D(\phi(\vect{p}'))^2} \\
						& \; \; \; \; \; \; \cdot \sum_{\vect{p}' \in A} \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \}
	\end{align*}
	Im ersten Summanden schätzen wir
	\[ \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \leq \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	nach oben ab, und im zweiten Summanden
	\[ \min \{ D(\phi(\vect{p}')), \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \} \leq D(\phi(\vect{p}')) \]
	Damit erhalten wir vereinfacht:
	\[ E[\theta(A)] \leq \frac{4}{\left|A\right|} \sum_{\vect{p} \in A} \sum_{\vect{p}' \in A} \EuclidSquared{\phi(\vect{p}')}{\phi(\vect{p})} \]
	Nun können wir Lemma~\ref{lemma:kkmpp-first-center-approx} anwenden und erhalten:
	\[ E[\theta(A)] \leq 8 \optpot(A) \]
\end{proof}
Aus den Lemmas~\ref{lemma:kkmpp-first-center-approx} und~\ref{lemma:kkmpp-remaining-center-approx} folgt, dass
die Wahl der Zentren von \kkmpp{} der Approximationseigenschaft genügt, solange der Algorithmus Zentren aus jedem Cluster
des optimalen Clusterings $C_\textrm{OPT}$ auswählt. Im dritten und letzten Schritt unserer Analyse zeigen wir nun,
dass der entstehende Fehler im anderen Fall nach oben insgesamt durch $\BigO{\log k}$ beschränkt ist.

\begin{lemma}
\label{lemma:kkmpp-error}
	Sei $C$ ein beliebiges Clustering. Gegeben seien $u > 0$ viele freie Cluster (die übrigen Cluster nennen wir abgedeckt)
	von $C_\textrm{OPT}$. $P_u$ seien die Punkte in
	diesen Clustern. Weiterhin sei $P_c = P - P_u$. Wenn $t \leq u$ zufällig gemäß der $D^2$-Gewichtung gewählte Zentren zu
	$C$ hinzugefügt werden, dann gilt für das erwartete neue Potenzial $E[\theta'(C)]$ von $C$:
	\[ E[\theta'(C)] \leq \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_t) + \frac{u-t}{u} \cdot \theta(P_u) \]
	Dabei ist $H_t$ die $t$-te harmonische Zahl $H_t = 1 + \frac{1}{2} + \dots + \frac{1}{t}$.
\end{lemma}

\begin{proof}
	Wir beweisen die Aussage durch vollständige Induktion für $(t-1, u)$ und $(t-1, u-1)$.
	\paragraph{Induktionsanfang.} Falls $t=0$ und $u > 0$, gilt:
	\[ 1 + H_t = \frac{u-t}{u} = 1 \]
	Wir betrachten nun den Fall, dass $t = u = 1$. Wir wählen genau ein neues Zentrum aus der Menge von genau einem freien Cluster
	mit der Wahrscheinlichkeit $\frac{\theta(P_u)}{\theta(C)}$. In diesem Fall folgt aus
	Lemma~\ref{lemma:kkmpp-remaining-center-approx}, dass
	\[ E[\theta'(C)] \leq \theta(P_c) + 8 \optpot(P_u) \]
	Auch wenn wir ein Zentrum aus einem abgedeckten Cluster wählen, gilt $\theta'(C) \leq \theta(C)$.
	Somit können wir das erwartete neue Potenzial in diesem Fall nach oben abschätzen:
	\begin{align*}
		E[\theta'(C)] &\leq \frac{\theta(P_u)}{\theta(C)} \cdot \left( \theta(P_c) + 8 \optpot(P_u) \right)
							+ \frac{\theta(P_u)}{\theta(C)} \cdot \theta(C) \\
					  &\leq 2 \theta(P_c) + 8 \optpot(P_u)
	\end{align*}
	Bezüglich der Behauptung gilt in diesem Fall, dass $1 + H_t = 1 + 1 = 2$ (und $\frac{u-t}{u} = \frac{1-1}{1} = 0$).
	Daher gilt die Behauptung auch in diesem Fall.
	\paragraph{Induktionsschritt.} Wir nehmen eine Fallunterscheidung bezüglich des Clusters vor, aus dem wir unser erstes
	Zentrum wählen.
	\begin{enumerate}
		\item[(a)] 	Wir nehmen an, das erste gewählte Zentrum stammt aus einem abgedeckten Cluster. Analog zum Induktionsanfang
					ist die Wahrscheinlichkeit dafür $\frac{\theta(P_c)}{\theta(C)}$. Da das Zentrum aus einem abgedeckten
					Cluster stammt, kann es $\theta(C)$ nicht vergrößern. Wir verwenden die Induktionsannahme mit $t-1$.
					Folglich tragen wir erwartet höchstens den folgenden Betrag zu $E[\theta'(C)]$ bei:
					\[ \frac{\theta(P_c)}{\theta(C)} \cdot \left( \left( \theta(P_c) + 8 \optpot(P_u) \right)
						\cdot (1 + H_{t-1}) + \frac{u - (t-1)}{u} \cdot \theta(P_u) \right) \]
		\item[(b)] 	Wir nehmen an, das erste gewählte Zentrum stammt aus einem freien Cluster $A$. Auch hier ist die
					Wahrscheinlichkeit für dieses Ereignis $\frac{\theta(A)}{\theta(C)}$. Für alle $a \in A$ sei
					$p_a$ die Wahrscheinlichkeit, dass wir $a$ als Zentrum wählen. Weiterhin sei $\theta_a$ das Potenzial
					$\theta(A)$, nachdem $a$ als Zentrum gewählt wurde. Wir wenden wieder die Induktionsannahme an, setzen
					jedoch $t = t-1$, $u = u-1$ und fügen $A$ zur Menge der abgedeckten Cluster hinzu. Dann tragen wir
					erwartet höchstens den folgenden Betrag zu $E[\theta'(C)]$ bei:
					\begin{align*}
						& \frac{\theta(A)}{\theta(C)} \cdot \sum_{a \in A} p_a \left(
							\left( \theta(P_c) + \theta_a + 8 \optpot(P_u) - 8 \optpot(A) \right)
							\right) \\
						& \; \; \; \cdot (1 + H_{t-1}) + \frac{u - (t-1)}{u-1} \cdot \left(
							\theta(P_u) - \theta(A) \right) \\
						&\leq \frac{\theta(A)}{\theta(C)} \cdot \bigg( \left( \theta(P_c) + 8 \optpot(P_u) \right)
							\cdot (1 + H_{t-1}) \\
						& \; \; \; + \frac{u-t}{u-1} \cdot \left( \theta(P_u) - \theta(A) \right) \bigg)
					\end{align*}
					Die zweite Abschätzung gilt, da $\sum_{a \in A} p_a \leq 8 \optpot(A)$ direkt aus
					Lemma~\ref{lemma:kkmpp-remaining-center-approx} folgt. An dieser Stelle müssten wir über alle freien
					Cluster summieren, um den gesamten Potenzial-Beitrag nach oben abzuschätzen. Dazu setzen wir die
					Power-Mean-Ungleichung ein. Mit dieser gilt:
					$ \sum_{A \subset P_u} \theta(A^2) \geq \frac{1}{u} \cdot \theta(P_u)^2 $.
					Damit ist es uns nun möglich, den über alle freien Cluster aufsummierten Beitrag zum Potenzial
					nach oben zu beschränken:
					\begin{align*}
						&\frac{\theta(P_u)}{\theta(C)} \cdot \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_{t-1}) \\
						& \; \; \; + \frac{1}{\theta(C)} \cdot \frac{u-t}{u-1} \cdot \left( 
						\theta(P_u)^2 - \frac{1}{u} \cdot \theta(P_u)^2 \right)
					\end{align*}
					Dieser Term lässt sich etwas vereinfachen zu:
					\[ \frac{\theta(P_u)}{\theta(C)} \cdot \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_{t-1})
						+ \frac{u-t}{u} \cdot \theta(P_u) \]
	\end{enumerate}
	Wir kombinieren nun den erwarteten Potenzial-Beitrag zu $\theta'(C)$ aus den Fällen (a) und (b) und erhalten die folgende
	obere Schranke:
	\begin{align*}
		E[\theta'(C)] &\leq \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot (1 + H_{t-1})
			+ \frac{u-t}{u} \cdot \theta(P_u) + \frac{\theta(P_c)}{\theta(C)} \cdot \frac{\theta(P_u)}{u} \\
			&\leq \left( \theta(P_c) + 8 \optpot(P_u) \right) \cdot \left( 1 + H_{t-1} + \frac{1}{u} \right)
				+ \frac{u-t}{u} \cdot \theta(P_u)
	\end{align*}
	In der Aussage von Lemma~\ref{lemma:kkmpp-error} ist $t \leq u$ vorausgesetzt. Daher gilt $\frac{1}{u} \leq \frac{1}{t}$.
	Damit folgt der Induktionsschluss.
\end{proof}
Wir können Satz~\ref{thm:kkmpp-ologk-approx} als Spezialfall von Lemma~\ref{lemma:kkmpp-error} beweisen. Dazu betrachten wir
das Clustering $C$, welches wir erhalten, nachdem die Zentren-Wahl von \kkmpp{} abgeschlossen ist und bevor der Kernel-$k$-means
Algorithmus ausgeführt wird. Sei $A$ das Cluster aus $C_\textrm{OPT}$, aus dem wir das erste Zentrum ausgewählt haben.
Wir wenden Lemma~\ref{lemma:kkmpp-error} mit $t = u = k-1$ an und legen dazu $A$ als einziges abgedecktes Cluster fest.
Dann gilt:
\[ E[\optpot] \leq \left( \theta(A) + 8 \optpot - 8 \optpot(A) \right) \cdot (1 + H_{k-1}) \]
Es ist allgemein bekannt, dass für $t \geq 2$ die $t$-te harmonische Zahl nach oben beschränkt ist durch $H_t < \ln{t} + 1$.
Damit gilt insbesondere $H_{k-1} \leq \ln{k} + 1$. Somit gilt:
\[ E[\optpot] \leq \left( \theta(A) + 8 \optpot - 8 \optpot(A) \right) \cdot (\ln{k} + 2) \]
Satz~\ref{thm:kkmpp-ologk-approx} folgt mit Lemma~\ref{lemma:kkmpp-first-center-approx}.

\subsection{Eine praktische Kernmengenkonstruktion}

In diesem Abschnitt wollen wir die Kernmengenkonstruktion von Feldman, Schmidt und Sohler~\cite{FeldmanSS13,Schmidt14} auf ihre
Implementierbarkeit untersuchen und gegebenenfalls für eine praktische Umsetzung modifizieren.